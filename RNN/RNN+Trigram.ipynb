{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucp31YKIG_k6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKskM9zQHOCy"
   },
   "outputs": [],
   "source": [
    "def get_data(train, test):\n",
    "  with open(train) as f:\n",
    "    sentences_train = f.read().split()\n",
    "    # f.read().split('.\\n'): split by new line\n",
    "    # f.read().split(): split by space\n",
    "  with open(test) as g:\n",
    "    sentences_test = g.read().split()\n",
    "\n",
    "  \"\"\"\n",
    "  for s in sentences_train[:10]: \n",
    "    print(s)\n",
    "  print()\n",
    "  for s in sentences_test[:10]: \n",
    "    print(s)\n",
    "  \"\"\"\n",
    "\n",
    "  train_all = \"\"\n",
    "  for i in sentences_train:\n",
    "    train_all += \" \" + i\n",
    "  train_all = train_all[1:]\n",
    "\n",
    "  test_all = \"\"\n",
    "  for i in sentences_test:\n",
    "    test_all += \" \" + i\n",
    "  test_all = test_all[1:]\n",
    "\n",
    "  \"\"\"\n",
    "  print(train_all[:20])\n",
    "  print()\n",
    "  print(test_all[:20])\n",
    "  print()\n",
    "  for i in train_all[:100].split():\n",
    "    print(i)\n",
    "  print()\n",
    "  \"\"\"\n",
    "\n",
    "  vocab = Counter(train_all.split())\n",
    "  # Counter() counts the instances of each element in your list \n",
    "  # print(vocab)\n",
    "  # print()\n",
    "\n",
    "  convert_list = list(vocab)\n",
    "  word2id = {w:i for i,w in enumerate(convert_list)}\n",
    "  # print(word2id)\n",
    "  # print()\n",
    "\n",
    "  train2id = []\n",
    "  for i in train_all.split():\n",
    "    train2id.append(word2id[i])\n",
    "  # print(train2id[:100])\n",
    "  # print()\n",
    "\n",
    "  test2id = []\n",
    "  for i in test_all.split():\n",
    "    test2id.append(word2id[i])\n",
    "  # print(test2id[:100])\n",
    "\n",
    "\n",
    "  return train2id, test2id, word2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYM12VROJA-F",
    "outputId": "3eae9ead-c48f-4d74-a36e-8e751a22cd6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  3,\n",
       "  7,\n",
       "  4,\n",
       "  13,\n",
       "  14,\n",
       "  3,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  10,\n",
       "  20,\n",
       "  13,\n",
       "  14,\n",
       "  3,\n",
       "  21,\n",
       "  7,\n",
       "  19,\n",
       "  8,\n",
       "  3,\n",
       "  22,\n",
       "  12,\n",
       "  23,\n",
       "  4,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  3,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  9,\n",
       "  30,\n",
       "  19,\n",
       "  31,\n",
       "  3,\n",
       "  32,\n",
       "  33,\n",
       "  12,\n",
       "  34,\n",
       "  35,\n",
       "  2,\n",
       "  36,\n",
       "  36,\n",
       "  1,\n",
       "  36,\n",
       "  36,\n",
       "  37,\n",
       "  35,\n",
       "  38,\n",
       "  5,\n",
       "  39,\n",
       "  37,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  14,\n",
       "  3,\n",
       "  44,\n",
       "  45,\n",
       "  12,\n",
       "  46,\n",
       "  19,\n",
       "  2,\n",
       "  47,\n",
       "  48,\n",
       "  2,\n",
       "  49,\n",
       "  50,\n",
       "  2,\n",
       "  34,\n",
       "  1,\n",
       "  51,\n",
       "  52,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  52,\n",
       "  4,\n",
       "  5,\n",
       "  53,\n",
       "  52,\n",
       "  28,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  19,\n",
       "  58,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  8,\n",
       "  59,\n",
       "  60,\n",
       "  10,\n",
       "  61,\n",
       "  62,\n",
       "  41,\n",
       "  5,\n",
       "  63,\n",
       "  12,\n",
       "  16,\n",
       "  14,\n",
       "  64,\n",
       "  65,\n",
       "  5,\n",
       "  66,\n",
       "  67,\n",
       "  14,\n",
       "  68,\n",
       "  19,\n",
       "  10,\n",
       "  69,\n",
       "  3,\n",
       "  16,\n",
       "  70,\n",
       "  13,\n",
       "  19,\n",
       "  71,\n",
       "  2,\n",
       "  34,\n",
       "  72,\n",
       "  19,\n",
       "  73,\n",
       "  74,\n",
       "  2,\n",
       "  72,\n",
       "  19,\n",
       "  73,\n",
       "  74,\n",
       "  71,\n",
       "  72,\n",
       "  19,\n",
       "  58,\n",
       "  71,\n",
       "  75,\n",
       "  25,\n",
       "  73,\n",
       "  5,\n",
       "  6,\n",
       "  76,\n",
       "  14,\n",
       "  77,\n",
       "  78,\n",
       "  65,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  12,\n",
       "  76,\n",
       "  19,\n",
       "  78,\n",
       "  75,\n",
       "  72,\n",
       "  19,\n",
       "  73,\n",
       "  74,\n",
       "  56,\n",
       "  79,\n",
       "  83,\n",
       "  84,\n",
       "  13,\n",
       "  85,\n",
       "  10,\n",
       "  86,\n",
       "  84,\n",
       "  5,\n",
       "  87,\n",
       "  77,\n",
       "  12,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  83,\n",
       "  91,\n",
       "  92,\n",
       "  76,\n",
       "  14,\n",
       "  77,\n",
       "  58,\n",
       "  88,\n",
       "  56,\n",
       "  79,\n",
       "  61,\n",
       "  12,\n",
       "  93,\n",
       "  94,\n",
       "  10,\n",
       "  8,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  3,\n",
       "  98,\n",
       "  10,\n",
       "  3,\n",
       "  99,\n",
       "  100,\n",
       "  25,\n",
       "  90,\n",
       "  101,\n",
       "  102,\n",
       "  14,\n",
       "  3,\n",
       "  103,\n",
       "  77,\n",
       "  8,\n",
       "  104,\n",
       "  12,\n",
       "  105,\n",
       "  4,\n",
       "  91,\n",
       "  94,\n",
       "  14,\n",
       "  5,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  12,\n",
       "  106,\n",
       "  107,\n",
       "  72,\n",
       "  19,\n",
       "  10,\n",
       "  72,\n",
       "  19,\n",
       "  73,\n",
       "  74,\n",
       "  55,\n",
       "  108,\n",
       "  97,\n",
       "  109,\n",
       "  86,\n",
       "  107,\n",
       "  12,\n",
       "  110,\n",
       "  2,\n",
       "  79,\n",
       "  111,\n",
       "  72,\n",
       "  75,\n",
       "  25,\n",
       "  73,\n",
       "  5,\n",
       "  112,\n",
       "  76,\n",
       "  4,\n",
       "  5,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  12,\n",
       "  90,\n",
       "  113,\n",
       "  72,\n",
       "  19,\n",
       "  73,\n",
       "  74,\n",
       "  10,\n",
       "  72,\n",
       "  19,\n",
       "  114,\n",
       "  113,\n",
       "  2,\n",
       "  34,\n",
       "  115,\n",
       "  1,\n",
       "  2,\n",
       "  115,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  116,\n",
       "  14,\n",
       "  117,\n",
       "  28,\n",
       "  115,\n",
       "  117,\n",
       "  4,\n",
       "  118,\n",
       "  25,\n",
       "  8,\n",
       "  5,\n",
       "  119,\n",
       "  120,\n",
       "  116,\n",
       "  58,\n",
       "  4,\n",
       "  121,\n",
       "  90,\n",
       "  122,\n",
       "  58,\n",
       "  117,\n",
       "  38,\n",
       "  123,\n",
       "  118,\n",
       "  25,\n",
       "  12,\n",
       "  23,\n",
       "  55,\n",
       "  124,\n",
       "  125,\n",
       "  90,\n",
       "  119,\n",
       "  126,\n",
       "  19,\n",
       "  78,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  117,\n",
       "  86,\n",
       "  130,\n",
       "  119,\n",
       "  117,\n",
       "  10,\n",
       "  69,\n",
       "  131,\n",
       "  132,\n",
       "  117,\n",
       "  31,\n",
       "  5,\n",
       "  115,\n",
       "  117,\n",
       "  116,\n",
       "  61,\n",
       "  1,\n",
       "  117,\n",
       "  12,\n",
       "  133,\n",
       "  71,\n",
       "  130,\n",
       "  126,\n",
       "  19,\n",
       "  78,\n",
       "  134,\n",
       "  115,\n",
       "  1,\n",
       "  20,\n",
       "  118,\n",
       "  23,\n",
       "  8,\n",
       "  120,\n",
       "  116,\n",
       "  19,\n",
       "  61,\n",
       "  129,\n",
       "  86,\n",
       "  135,\n",
       "  117,\n",
       "  12,\n",
       "  13,\n",
       "  94,\n",
       "  14,\n",
       "  105,\n",
       "  4,\n",
       "  3,\n",
       "  136,\n",
       "  137,\n",
       "  25,\n",
       "  138,\n",
       "  115,\n",
       "  139,\n",
       "  120,\n",
       "  138,\n",
       "  12,\n",
       "  34,\n",
       "  140,\n",
       "  19,\n",
       "  14,\n",
       "  3,\n",
       "  35,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  19,\n",
       "  2,\n",
       "  105,\n",
       "  145,\n",
       "  146,\n",
       "  19,\n",
       "  3,\n",
       "  140,\n",
       "  19,\n",
       "  14,\n",
       "  3,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  19,\n",
       "  10,\n",
       "  35,\n",
       "  12,\n",
       "  3,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  4,\n",
       "  5,\n",
       "  147,\n",
       "  14,\n",
       "  148,\n",
       "  41,\n",
       "  149,\n",
       "  19,\n",
       "  10,\n",
       "  86,\n",
       "  77,\n",
       "  58,\n",
       "  150,\n",
       "  151,\n",
       "  10,\n",
       "  90,\n",
       "  152,\n",
       "  90,\n",
       "  3,\n",
       "  35,\n",
       "  141,\n",
       "  153,\n",
       "  154,\n",
       "  12,\n",
       "  77,\n",
       "  8,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  42,\n",
       "  78,\n",
       "  88,\n",
       "  159,\n",
       "  157,\n",
       "  113,\n",
       "  160,\n",
       "  93,\n",
       "  3,\n",
       "  161,\n",
       "  10,\n",
       "  20,\n",
       "  155,\n",
       "  162,\n",
       "  163,\n",
       "  19,\n",
       "  5,\n",
       "  164,\n",
       "  165,\n",
       "  14,\n",
       "  149,\n",
       "  19,\n",
       "  166,\n",
       "  25,\n",
       "  42,\n",
       "  3,\n",
       "  167,\n",
       "  14,\n",
       "  158,\n",
       "  19,\n",
       "  88,\n",
       "  168,\n",
       "  12,\n",
       "  3,\n",
       "  144,\n",
       "  19,\n",
       "  169,\n",
       "  42,\n",
       "  35,\n",
       "  170,\n",
       "  35,\n",
       "  8,\n",
       "  171,\n",
       "  12,\n",
       "  42,\n",
       "  172,\n",
       "  35,\n",
       "  10,\n",
       "  35,\n",
       "  10,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  3,\n",
       "  176,\n",
       "  162,\n",
       "  177,\n",
       "  8,\n",
       "  3,\n",
       "  178,\n",
       "  12,\n",
       "  179,\n",
       "  175,\n",
       "  3,\n",
       "  180,\n",
       "  141,\n",
       "  181,\n",
       "  42,\n",
       "  182,\n",
       "  35,\n",
       "  10,\n",
       "  35,\n",
       "  12,\n",
       "  174,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  3,\n",
       "  161,\n",
       "  42,\n",
       "  186,\n",
       "  35,\n",
       "  12,\n",
       "  187,\n",
       "  162,\n",
       "  19,\n",
       "  2,\n",
       "  3,\n",
       "  188,\n",
       "  41,\n",
       "  189,\n",
       "  146,\n",
       "  19,\n",
       "  102,\n",
       "  3,\n",
       "  162,\n",
       "  19,\n",
       "  78,\n",
       "  65,\n",
       "  190,\n",
       "  191,\n",
       "  5,\n",
       "  192,\n",
       "  193,\n",
       "  12,\n",
       "  194,\n",
       "  2,\n",
       "  34,\n",
       "  1,\n",
       "  195,\n",
       "  2,\n",
       "  1,\n",
       "  195,\n",
       "  4,\n",
       "  5,\n",
       "  196,\n",
       "  19,\n",
       "  195,\n",
       "  8,\n",
       "  1,\n",
       "  10,\n",
       "  197,\n",
       "  198,\n",
       "  12,\n",
       "  199,\n",
       "  22,\n",
       "  200,\n",
       "  2,\n",
       "  3,\n",
       "  195,\n",
       "  157,\n",
       "  201,\n",
       "  3,\n",
       "  35,\n",
       "  199,\n",
       "  22,\n",
       "  200,\n",
       "  12,\n",
       "  4,\n",
       "  13,\n",
       "  14,\n",
       "  3,\n",
       "  35,\n",
       "  195,\n",
       "  19,\n",
       "  28,\n",
       "  22,\n",
       "  200,\n",
       "  202,\n",
       "  157,\n",
       "  113,\n",
       "  128,\n",
       "  25,\n",
       "  12,\n",
       "  35,\n",
       "  22,\n",
       "  200,\n",
       "  203,\n",
       "  19,\n",
       "  102,\n",
       "  204,\n",
       "  19,\n",
       "  71,\n",
       "  205,\n",
       "  8,\n",
       "  197,\n",
       "  206,\n",
       "  207,\n",
       "  204,\n",
       "  12,\n",
       "  34,\n",
       "  208,\n",
       "  209,\n",
       "  10,\n",
       "  210,\n",
       "  2,\n",
       "  208,\n",
       "  209,\n",
       "  4,\n",
       "  5,\n",
       "  209,\n",
       "  8,\n",
       "  3,\n",
       "  211,\n",
       "  12,\n",
       "  212,\n",
       "  12,\n",
       "  156,\n",
       "  14,\n",
       "  210,\n",
       "  12,\n",
       "  213,\n",
       "  14,\n",
       "  35,\n",
       "  10,\n",
       "  35,\n",
       "  77,\n",
       "  214,\n",
       "  25,\n",
       "  133,\n",
       "  12,\n",
       "  3,\n",
       "  209,\n",
       "  215,\n",
       "  4,\n",
       "  1,\n",
       "  12,\n",
       "  34,\n",
       "  216,\n",
       "  217,\n",
       "  2,\n",
       "  5,\n",
       "  216,\n",
       "  217,\n",
       "  86,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  218,\n",
       "  58,\n",
       "  4,\n",
       "  219,\n",
       "  220,\n",
       "  25,\n",
       "  90,\n",
       "  5,\n",
       "  53,\n",
       "  12,\n",
       "  23,\n",
       "  4,\n",
       "  221,\n",
       "  25,\n",
       "  90,\n",
       "  222,\n",
       "  223,\n",
       "  90,\n",
       "  224,\n",
       "  216,\n",
       "  86,\n",
       "  90,\n",
       "  225,\n",
       "  223,\n",
       "  226,\n",
       "  23,\n",
       "  12,\n",
       "  227,\n",
       "  77,\n",
       "  221,\n",
       "  23,\n",
       "  90,\n",
       "  56,\n",
       "  1,\n",
       "  19,\n",
       "  12,\n",
       "  216,\n",
       "  217,\n",
       "  19,\n",
       "  71,\n",
       "  121,\n",
       "  226,\n",
       "  228,\n",
       "  217,\n",
       "  19,\n",
       "  8,\n",
       "  58,\n",
       "  3,\n",
       "  229,\n",
       "  90,\n",
       "  3,\n",
       "  223,\n",
       "  4,\n",
       "  1,\n",
       "  12,\n",
       "  105,\n",
       "  230,\n",
       "  19,\n",
       "  90,\n",
       "  225,\n",
       "  5,\n",
       "  164,\n",
       "  231,\n",
       "  14,\n",
       "  223,\n",
       "  10,\n",
       "  102,\n",
       "  3,\n",
       "  223,\n",
       "  232,\n",
       "  23,\n",
       "  233,\n",
       "  19,\n",
       "  90,\n",
       "  113,\n",
       "  225,\n",
       "  213,\n",
       "  234,\n",
       "  12,\n",
       "  124,\n",
       "  216,\n",
       "  217,\n",
       "  19,\n",
       "  71,\n",
       "  235,\n",
       "  10,\n",
       "  236,\n",
       "  25,\n",
       "  90,\n",
       "  228,\n",
       "  217,\n",
       "  19,\n",
       "  12,\n",
       "  237,\n",
       "  238,\n",
       "  19,\n",
       "  239,\n",
       "  226,\n",
       "  240,\n",
       "  19,\n",
       "  14,\n",
       "  1,\n",
       "  19,\n",
       "  90,\n",
       "  241,\n",
       "  5,\n",
       "  1,\n",
       "  10,\n",
       "  242,\n",
       "  1,\n",
       "  12,\n",
       "  34,\n",
       "  243,\n",
       "  244,\n",
       "  1,\n",
       "  2,\n",
       "  243,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  47,\n",
       "  248,\n",
       "  249,\n",
       "  244,\n",
       "  250,\n",
       "  1,\n",
       "  251,\n",
       "  12,\n",
       "  23,\n",
       "  38,\n",
       "  252,\n",
       "  25,\n",
       "  42,\n",
       "  172,\n",
       "  35,\n",
       "  10,\n",
       "  35,\n",
       "  47,\n",
       "  1,\n",
       "  12,\n",
       "  3,\n",
       "  247,\n",
       "  253,\n",
       "  25,\n",
       "  254,\n",
       "  12,\n",
       "  35,\n",
       "  42,\n",
       "  3,\n",
       "  255,\n",
       "  256,\n",
       "  247,\n",
       "  257,\n",
       "  19,\n",
       "  20,\n",
       "  254,\n",
       "  12,\n",
       "  35,\n",
       "  42,\n",
       "  3,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  257,\n",
       "  19,\n",
       "  12,\n",
       "  262,\n",
       "  263,\n",
       "  2,\n",
       "  264,\n",
       "  138,\n",
       "  265,\n",
       "  5,\n",
       "  152,\n",
       "  152,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  264,\n",
       "  138,\n",
       "  266,\n",
       "  267,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  264,\n",
       "  138,\n",
       "  1,\n",
       "  8,\n",
       "  3,\n",
       "  268,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  264,\n",
       "  138,\n",
       "  272,\n",
       "  19,\n",
       "  273,\n",
       "  160,\n",
       "  93,\n",
       "  274,\n",
       "  275,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  264,\n",
       "  138,\n",
       "  276,\n",
       "  267,\n",
       "  1,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  264,\n",
       "  138,\n",
       "  1,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  264,\n",
       "  138,\n",
       "  277,\n",
       "  278,\n",
       "  58,\n",
       "  279,\n",
       "  41,\n",
       "  93,\n",
       "  280,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  264,\n",
       "  138,\n",
       "  281,\n",
       "  89,\n",
       "  274,\n",
       "  275,\n",
       "  282,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  264,\n",
       "  138,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  264,\n",
       "  138,\n",
       "  117,\n",
       "  286,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  264,\n",
       "  138,\n",
       "  287,\n",
       "  42,\n",
       "  265,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  264,\n",
       "  138,\n",
       "  288,\n",
       "  5,\n",
       "  1,\n",
       "  289,\n",
       "  138,\n",
       "  244,\n",
       "  35,\n",
       "  34,\n",
       "  290,\n",
       "  209,\n",
       "  10,\n",
       "  291,\n",
       "  292,\n",
       "  2,\n",
       "  290,\n",
       "  209,\n",
       "  4,\n",
       "  5,\n",
       "  209,\n",
       "  8,\n",
       "  3,\n",
       "  211,\n",
       "  12,\n",
       "  212,\n",
       "  12,\n",
       "  156,\n",
       "  14,\n",
       "  291,\n",
       "  292,\n",
       "  12,\n",
       "  213,\n",
       "  14,\n",
       "  3,\n",
       "  35,\n",
       "  293,\n",
       "  10,\n",
       "  3,\n",
       "  294,\n",
       "  38,\n",
       "  35,\n",
       "  10,\n",
       "  62,\n",
       "  41,\n",
       "  23,\n",
       "  3,\n",
       "  295,\n",
       "  244,\n",
       "  296,\n",
       "  297,\n",
       "  209,\n",
       "  8,\n",
       "  291,\n",
       "  292,\n",
       "  12,\n",
       "  298,\n",
       "  19,\n",
       "  ...],\n",
       " [3,\n",
       "  631,\n",
       "  19,\n",
       "  2,\n",
       "  3,\n",
       "  631,\n",
       "  19,\n",
       "  311,\n",
       "  5,\n",
       "  3167,\n",
       "  190,\n",
       "  1,\n",
       "  136,\n",
       "  680,\n",
       "  1124,\n",
       "  249,\n",
       "  38,\n",
       "  3662,\n",
       "  1,\n",
       "  10,\n",
       "  5,\n",
       "  328,\n",
       "  1972,\n",
       "  12,\n",
       "  544,\n",
       "  8,\n",
       "  1589,\n",
       "  10,\n",
       "  1,\n",
       "  65,\n",
       "  19,\n",
       "  219,\n",
       "  656,\n",
       "  1,\n",
       "  25,\n",
       "  93,\n",
       "  221,\n",
       "  41,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  97,\n",
       "  4321,\n",
       "  19,\n",
       "  20,\n",
       "  8,\n",
       "  325,\n",
       "  702,\n",
       "  19,\n",
       "  12,\n",
       "  247,\n",
       "  19,\n",
       "  2,\n",
       "  88,\n",
       "  252,\n",
       "  25,\n",
       "  132,\n",
       "  349,\n",
       "  247,\n",
       "  10,\n",
       "  138,\n",
       "  426,\n",
       "  3815,\n",
       "  1765,\n",
       "  138,\n",
       "  10,\n",
       "  8,\n",
       "  35,\n",
       "  10,\n",
       "  3,\n",
       "  247,\n",
       "  19,\n",
       "  783,\n",
       "  19,\n",
       "  311,\n",
       "  138,\n",
       "  65,\n",
       "  19,\n",
       "  23,\n",
       "  379,\n",
       "  90,\n",
       "  105,\n",
       "  138,\n",
       "  10,\n",
       "  138,\n",
       "  664,\n",
       "  19,\n",
       "  2491,\n",
       "  57,\n",
       "  19,\n",
       "  1371,\n",
       "  138,\n",
       "  10,\n",
       "  138,\n",
       "  56,\n",
       "  278,\n",
       "  1,\n",
       "  1,\n",
       "  138,\n",
       "  10,\n",
       "  138,\n",
       "  3,\n",
       "  1,\n",
       "  14,\n",
       "  23,\n",
       "  102,\n",
       "  138,\n",
       "  20,\n",
       "  138,\n",
       "  804,\n",
       "  828,\n",
       "  2750,\n",
       "  19,\n",
       "  12,\n",
       "  298,\n",
       "  19,\n",
       "  1138,\n",
       "  247,\n",
       "  10,\n",
       "  137,\n",
       "  25,\n",
       "  138,\n",
       "  5,\n",
       "  1154,\n",
       "  56,\n",
       "  278,\n",
       "  379,\n",
       "  93,\n",
       "  2591,\n",
       "  138,\n",
       "  352,\n",
       "  427,\n",
       "  8,\n",
       "  35,\n",
       "  20,\n",
       "  4,\n",
       "  132,\n",
       "  16,\n",
       "  1445,\n",
       "  247,\n",
       "  12,\n",
       "  102,\n",
       "  3,\n",
       "  702,\n",
       "  19,\n",
       "  42,\n",
       "  3,\n",
       "  247,\n",
       "  793,\n",
       "  853,\n",
       "  5,\n",
       "  1037,\n",
       "  677,\n",
       "  5,\n",
       "  475,\n",
       "  288,\n",
       "  5,\n",
       "  4312,\n",
       "  1473,\n",
       "  78,\n",
       "  1023,\n",
       "  2575,\n",
       "  35,\n",
       "  10,\n",
       "  288,\n",
       "  130,\n",
       "  57,\n",
       "  19,\n",
       "  108,\n",
       "  41,\n",
       "  1052,\n",
       "  100,\n",
       "  41,\n",
       "  90,\n",
       "  163,\n",
       "  3,\n",
       "  1473,\n",
       "  3133,\n",
       "  25,\n",
       "  12,\n",
       "  3,\n",
       "  247,\n",
       "  490,\n",
       "  19,\n",
       "  298,\n",
       "  19,\n",
       "  16,\n",
       "  1445,\n",
       "  783,\n",
       "  19,\n",
       "  138,\n",
       "  2906,\n",
       "  69,\n",
       "  54,\n",
       "  580,\n",
       "  23,\n",
       "  138,\n",
       "  20,\n",
       "  138,\n",
       "  740,\n",
       "  410,\n",
       "  1382,\n",
       "  19,\n",
       "  138,\n",
       "  303,\n",
       "  175,\n",
       "  167,\n",
       "  35,\n",
       "  8,\n",
       "  3,\n",
       "  257,\n",
       "  19,\n",
       "  12,\n",
       "  3,\n",
       "  247,\n",
       "  486,\n",
       "  38,\n",
       "  368,\n",
       "  25,\n",
       "  138,\n",
       "  3,\n",
       "  1,\n",
       "  581,\n",
       "  90,\n",
       "  62,\n",
       "  91,\n",
       "  1882,\n",
       "  2817,\n",
       "  138,\n",
       "  303,\n",
       "  38,\n",
       "  252,\n",
       "  25,\n",
       "  8,\n",
       "  35,\n",
       "  12,\n",
       "  3,\n",
       "  247,\n",
       "  4,\n",
       "  734,\n",
       "  677,\n",
       "  3,\n",
       "  160,\n",
       "  20,\n",
       "  1645,\n",
       "  57,\n",
       "  19,\n",
       "  14,\n",
       "  1395,\n",
       "  17,\n",
       "  12,\n",
       "  3,\n",
       "  247,\n",
       "  321,\n",
       "  58,\n",
       "  10,\n",
       "  137,\n",
       "  25,\n",
       "  138,\n",
       "  3831,\n",
       "  4,\n",
       "  1,\n",
       "  25,\n",
       "  138,\n",
       "  352,\n",
       "  427,\n",
       "  8,\n",
       "  35,\n",
       "  12,\n",
       "  179,\n",
       "  4,\n",
       "  152,\n",
       "  41,\n",
       "  90,\n",
       "  2642,\n",
       "  427,\n",
       "  13,\n",
       "  871,\n",
       "  247,\n",
       "  10,\n",
       "  137,\n",
       "  25,\n",
       "  138,\n",
       "  53,\n",
       "  19,\n",
       "  20,\n",
       "  2139,\n",
       "  19,\n",
       "  138,\n",
       "  10,\n",
       "  69,\n",
       "  298,\n",
       "  19,\n",
       "  252,\n",
       "  870,\n",
       "  4,\n",
       "  2753,\n",
       "  12,\n",
       "  34,\n",
       "  1,\n",
       "  1977,\n",
       "  2,\n",
       "  1,\n",
       "  1736,\n",
       "  1,\n",
       "  1977,\n",
       "  10,\n",
       "  380,\n",
       "  213,\n",
       "  1,\n",
       "  1977,\n",
       "  10,\n",
       "  4,\n",
       "  5,\n",
       "  328,\n",
       "  1133,\n",
       "  12,\n",
       "  179,\n",
       "  38,\n",
       "  3,\n",
       "  1308,\n",
       "  14,\n",
       "  156,\n",
       "  93,\n",
       "  1377,\n",
       "  1203,\n",
       "  226,\n",
       "  172,\n",
       "  35,\n",
       "  90,\n",
       "  182,\n",
       "  35,\n",
       "  10,\n",
       "  20,\n",
       "  38,\n",
       "  5,\n",
       "  348,\n",
       "  14,\n",
       "  816,\n",
       "  93,\n",
       "  1,\n",
       "  20,\n",
       "  3,\n",
       "  480,\n",
       "  226,\n",
       "  35,\n",
       "  847,\n",
       "  35,\n",
       "  12,\n",
       "  226,\n",
       "  35,\n",
       "  90,\n",
       "  35,\n",
       "  10,\n",
       "  1977,\n",
       "  38,\n",
       "  2004,\n",
       "  14,\n",
       "  156,\n",
       "  26,\n",
       "  3,\n",
       "  2376,\n",
       "  93,\n",
       "  1377,\n",
       "  1203,\n",
       "  20,\n",
       "  2004,\n",
       "  14,\n",
       "  156,\n",
       "  93,\n",
       "  198,\n",
       "  10,\n",
       "  20,\n",
       "  42,\n",
       "  35,\n",
       "  916,\n",
       "  35,\n",
       "  179,\n",
       "  38,\n",
       "  2321,\n",
       "  25,\n",
       "  2004,\n",
       "  14,\n",
       "  156,\n",
       "  93,\n",
       "  2210,\n",
       "  19,\n",
       "  12,\n",
       "  1977,\n",
       "  38,\n",
       "  5,\n",
       "  162,\n",
       "  8,\n",
       "  3,\n",
       "  35,\n",
       "  1130,\n",
       "  2542,\n",
       "  561,\n",
       "  12,\n",
       "  42,\n",
       "  35,\n",
       "  723,\n",
       "  35,\n",
       "  10,\n",
       "  1977,\n",
       "  509,\n",
       "  25,\n",
       "  179,\n",
       "  191,\n",
       "  936,\n",
       "  25,\n",
       "  226,\n",
       "  3,\n",
       "  1130,\n",
       "  142,\n",
       "  20,\n",
       "  183,\n",
       "  2182,\n",
       "  810,\n",
       "  213,\n",
       "  91,\n",
       "  4021,\n",
       "  26,\n",
       "  3,\n",
       "  1138,\n",
       "  831,\n",
       "  561,\n",
       "  12,\n",
       "  179,\n",
       "  157,\n",
       "  2182,\n",
       "  213,\n",
       "  91,\n",
       "  1132,\n",
       "  162,\n",
       "  93,\n",
       "  2258,\n",
       "  14,\n",
       "  1378,\n",
       "  8,\n",
       "  3,\n",
       "  35,\n",
       "  561,\n",
       "  12,\n",
       "  34,\n",
       "  392,\n",
       "  2260,\n",
       "  1581,\n",
       "  2,\n",
       "  392,\n",
       "  19,\n",
       "  1581,\n",
       "  4,\n",
       "  5,\n",
       "  783,\n",
       "  244,\n",
       "  522,\n",
       "  349,\n",
       "  85,\n",
       "  1043,\n",
       "  244,\n",
       "  2473,\n",
       "  51,\n",
       "  52,\n",
       "  139,\n",
       "  47,\n",
       "  3051,\n",
       "  1055,\n",
       "  19,\n",
       "  93,\n",
       "  939,\n",
       "  3227,\n",
       "  10,\n",
       "  937,\n",
       "  35,\n",
       "  20,\n",
       "  942,\n",
       "  35,\n",
       "  12,\n",
       "  3425,\n",
       "  2,\n",
       "  3,\n",
       "  3425,\n",
       "  14,\n",
       "  392,\n",
       "  19,\n",
       "  1581,\n",
       "  188,\n",
       "  19,\n",
       "  671,\n",
       "  10,\n",
       "  5,\n",
       "  348,\n",
       "  14,\n",
       "  5,\n",
       "  76,\n",
       "  137,\n",
       "  25,\n",
       "  3,\n",
       "  3871,\n",
       "  19,\n",
       "  12,\n",
       "  3,\n",
       "  3871,\n",
       "  19,\n",
       "  1280,\n",
       "  57,\n",
       "  19,\n",
       "  226,\n",
       "  85,\n",
       "  90,\n",
       "  85,\n",
       "  213,\n",
       "  91,\n",
       "  1888,\n",
       "  586,\n",
       "  10,\n",
       "  213,\n",
       "  3,\n",
       "  63,\n",
       "  1788,\n",
       "  19,\n",
       "  42,\n",
       "  3,\n",
       "  2030,\n",
       "  19,\n",
       "  5,\n",
       "  844,\n",
       "  12,\n",
       "  671,\n",
       "  19,\n",
       "  1484,\n",
       "  4,\n",
       "  2206,\n",
       "  25,\n",
       "  14,\n",
       "  101,\n",
       "  41,\n",
       "  5,\n",
       "  162,\n",
       "  93,\n",
       "  2258,\n",
       "  14,\n",
       "  3,\n",
       "  63,\n",
       "  10,\n",
       "  20,\n",
       "  671,\n",
       "  1426,\n",
       "  19,\n",
       "  427,\n",
       "  298,\n",
       "  19,\n",
       "  231,\n",
       "  14,\n",
       "  5,\n",
       "  1189,\n",
       "  90,\n",
       "  101,\n",
       "  3,\n",
       "  3871,\n",
       "  19,\n",
       "  12,\n",
       "  3,\n",
       "  2238,\n",
       "  3400,\n",
       "  667,\n",
       "  19,\n",
       "  243,\n",
       "  1395,\n",
       "  91,\n",
       "  4378,\n",
       "  244,\n",
       "  3871,\n",
       "  2808,\n",
       "  1,\n",
       "  342,\n",
       "  41,\n",
       "  93,\n",
       "  3,\n",
       "  495,\n",
       "  10,\n",
       "  20,\n",
       "  671,\n",
       "  3220,\n",
       "  19,\n",
       "  1108,\n",
       "  1484,\n",
       "  226,\n",
       "  1395,\n",
       "  1226,\n",
       "  25,\n",
       "  12,\n",
       "  34,\n",
       "  829,\n",
       "  19,\n",
       "  14,\n",
       "  3159,\n",
       "  3160,\n",
       "  2,\n",
       "  8,\n",
       "  3159,\n",
       "  3160,\n",
       "  10,\n",
       "  829,\n",
       "  19,\n",
       "  71,\n",
       "  3,\n",
       "  662,\n",
       "  244,\n",
       "  1782,\n",
       "  1904,\n",
       "  2002,\n",
       "  19,\n",
       "  12,\n",
       "  3,\n",
       "  349,\n",
       "  1782,\n",
       "  71,\n",
       "  3,\n",
       "  827,\n",
       "  19,\n",
       "  12,\n",
       "  155,\n",
       "  827,\n",
       "  1668,\n",
       "  19,\n",
       "  5,\n",
       "  167,\n",
       "  14,\n",
       "  829,\n",
       "  19,\n",
       "  12,\n",
       "  133,\n",
       "  71,\n",
       "  35,\n",
       "  829,\n",
       "  19,\n",
       "  8,\n",
       "  3,\n",
       "  319,\n",
       "  12,\n",
       "  88,\n",
       "  71,\n",
       "  3137,\n",
       "  25,\n",
       "  31,\n",
       "  35,\n",
       "  827,\n",
       "  19,\n",
       "  12,\n",
       "  155,\n",
       "  829,\n",
       "  4,\n",
       "  3063,\n",
       "  25,\n",
       "  742,\n",
       "  5,\n",
       "  829,\n",
       "  1308,\n",
       "  12,\n",
       "  1308,\n",
       "  19,\n",
       "  71,\n",
       "  2321,\n",
       "  25,\n",
       "  47,\n",
       "  3,\n",
       "  388,\n",
       "  495,\n",
       "  12,\n",
       "  829,\n",
       "  19,\n",
       "  2,\n",
       "  102,\n",
       "  294,\n",
       "  167,\n",
       "  19,\n",
       "  71,\n",
       "  226,\n",
       "  3,\n",
       "  16,\n",
       "  717,\n",
       "  293,\n",
       "  14,\n",
       "  3159,\n",
       "  3160,\n",
       "  10,\n",
       "  8,\n",
       "  35,\n",
       "  12,\n",
       "  294,\n",
       "  2113,\n",
       "  65,\n",
       "  19,\n",
       "  656,\n",
       "  1400,\n",
       "  25,\n",
       "  221,\n",
       "  41,\n",
       "  3,\n",
       "  728,\n",
       "  488,\n",
       "  1564,\n",
       "  416,\n",
       "  3,\n",
       "  2137,\n",
       "  488,\n",
       "  12,\n",
       "  34,\n",
       "  630,\n",
       "  2,\n",
       "  5,\n",
       "  630,\n",
       "  4,\n",
       "  5,\n",
       "  1169,\n",
       "  348,\n",
       "  14,\n",
       "  5,\n",
       "  1843,\n",
       "  1035,\n",
       "  12,\n",
       "  5,\n",
       "  1345,\n",
       "  630,\n",
       "  4,\n",
       "  137,\n",
       "  25,\n",
       "  5,\n",
       "  2700,\n",
       "  12,\n",
       "  155,\n",
       "  1843,\n",
       "  1035,\n",
       "  65,\n",
       "  19,\n",
       "  298,\n",
       "  19,\n",
       "  1258,\n",
       "  657,\n",
       "  19,\n",
       "  362,\n",
       "  41,\n",
       "  78,\n",
       "  4,\n",
       "  137,\n",
       "  25,\n",
       "  5,\n",
       "  630,\n",
       "  12,\n",
       "  8,\n",
       "  16,\n",
       "  14,\n",
       "  64,\n",
       "  1035,\n",
       "  19,\n",
       "  10,\n",
       "  3,\n",
       "  1682,\n",
       "  14,\n",
       "  5,\n",
       "  661,\n",
       "  86,\n",
       "  5,\n",
       "  2237,\n",
       "  4,\n",
       "  5,\n",
       "  630,\n",
       "  12,\n",
       "  8,\n",
       "  369,\n",
       "  1035,\n",
       "  19,\n",
       "  10,\n",
       "  3,\n",
       "  1682,\n",
       "  14,\n",
       "  5,\n",
       "  630,\n",
       "  4,\n",
       "  124,\n",
       "  5,\n",
       "  630,\n",
       "  12,\n",
       "  8,\n",
       "  1,\n",
       "  20,\n",
       "  1,\n",
       "  10,\n",
       "  3,\n",
       "  2326,\n",
       "  14,\n",
       "  3,\n",
       "  319,\n",
       "  4,\n",
       "  137,\n",
       "  25,\n",
       "  3,\n",
       "  630,\n",
       "  12,\n",
       "  34,\n",
       "  2401,\n",
       "  2,\n",
       "  5,\n",
       "  2401,\n",
       "  4,\n",
       "  5,\n",
       "  1180,\n",
       "  1904,\n",
       "  2728,\n",
       "  1,\n",
       "  41,\n",
       "  90,\n",
       "  91,\n",
       "  1865,\n",
       "  86,\n",
       "  3039,\n",
       "  1206,\n",
       "  10,\n",
       "  86,\n",
       "  2021,\n",
       "  12,\n",
       "  23,\n",
       "  4,\n",
       "  3,\n",
       "  1503,\n",
       "  14,\n",
       "  5,\n",
       "  1593,\n",
       "  10,\n",
       "  5,\n",
       "  497,\n",
       "  10,\n",
       "  86,\n",
       "  130,\n",
       "  1540,\n",
       "  19,\n",
       "  14,\n",
       "  821,\n",
       "  1312,\n",
       "  12,\n",
       "  2401,\n",
       "  19,\n",
       "  8,\n",
       "  3,\n",
       "  22,\n",
       "  2,\n",
       "  325,\n",
       "  319,\n",
       "  19,\n",
       "  65,\n",
       "  1939,\n",
       "  25,\n",
       "  3,\n",
       "  368,\n",
       "  14,\n",
       "  2401,\n",
       "  93,\n",
       "  132,\n",
       "  1180,\n",
       "  20,\n",
       "  1392,\n",
       "  1904,\n",
       "  1208,\n",
       "  12,\n",
       "  369,\n",
       "  319,\n",
       "  19,\n",
       "  71,\n",
       "  96,\n",
       "  10,\n",
       "  1491,\n",
       "  10,\n",
       "  3310,\n",
       "  10,\n",
       "  3351,\n",
       "  10,\n",
       "  1001,\n",
       "  10,\n",
       "  610,\n",
       "  10,\n",
       "  473,\n",
       "  10,\n",
       "  2324,\n",
       "  10,\n",
       "  1004,\n",
       "  10,\n",
       "  1002,\n",
       "  10,\n",
       "  998,\n",
       "  20,\n",
       "  11,\n",
       "  12,\n",
       "  34,\n",
       "  1926,\n",
       "  3158,\n",
       "  1543,\n",
       "  2,\n",
       "  3,\n",
       "  1926,\n",
       "  3158,\n",
       "  1543,\n",
       "  4,\n",
       "  5,\n",
       "  196,\n",
       "  19,\n",
       "  575,\n",
       "  8,\n",
       "  1926,\n",
       "  12,\n",
       "  325,\n",
       "  520,\n",
       "  1,\n",
       "  46,\n",
       "  19,\n",
       "  134,\n",
       "  48,\n",
       "  133,\n",
       "  12,\n",
       "  64,\n",
       "  196,\n",
       "  19,\n",
       "  490,\n",
       "  3158,\n",
       "  10,\n",
       "  3350,\n",
       "  10,\n",
       "  20,\n",
       "  3,\n",
       "  1101,\n",
       "  452,\n",
       "  453,\n",
       "  12,\n",
       "  34,\n",
       "  2285,\n",
       "  1,\n",
       "  2,\n",
       "  2285,\n",
       "  1,\n",
       "  38,\n",
       "  91,\n",
       "  248,\n",
       "  244,\n",
       "  544,\n",
       "  1,\n",
       "  20,\n",
       "  4875,\n",
       "  12,\n",
       "  834,\n",
       "  4,\n",
       "  991,\n",
       "  380,\n",
       "  93,\n",
       "  636,\n",
       "  3097,\n",
       "  19,\n",
       "  20,\n",
       "  1238,\n",
       "  19,\n",
       "  834,\n",
       "  139,\n",
       "  97,\n",
       "  3,\n",
       "  35,\n",
       "  19,\n",
       "  20,\n",
       "  35,\n",
       "  19,\n",
       "  12,\n",
       "  1727,\n",
       "  1238,\n",
       "  19,\n",
       "  191,\n",
       "  1128,\n",
       "  2213,\n",
       "  19,\n",
       "  12,\n",
       "  1,\n",
       "  38,\n",
       "  544,\n",
       "  8,\n",
       "  1653,\n",
       "  10,\n",
       "  2035,\n",
       "  12,\n",
       "  1227,\n",
       "  12,\n",
       "  12,\n",
       "  8,\n",
       "  35,\n",
       "  834,\n",
       "  1036,\n",
       "  25,\n",
       "  90,\n",
       "  292,\n",
       "  10,\n",
       "  28,\n",
       "  834,\n",
       "  214,\n",
       "  ...],\n",
       " {'jet': 0,\n",
       "  '<UNK>': 1,\n",
       "  ':': 2,\n",
       "  'the': 3,\n",
       "  'is': 4,\n",
       "  'a': 5,\n",
       "  'large': 6,\n",
       "  'fountain': 7,\n",
       "  'in': 8,\n",
       "  'geneva': 9,\n",
       "  ',': 10,\n",
       "  'switzerland': 11,\n",
       "  '.': 12,\n",
       "  'one': 13,\n",
       "  'of': 14,\n",
       "  'citys': 15,\n",
       "  'most': 16,\n",
       "  'famous': 17,\n",
       "  'landmark': 18,\n",
       "  '_s': 19,\n",
       "  'and': 20,\n",
       "  'largest': 21,\n",
       "  'world': 22,\n",
       "  'it': 23,\n",
       "  'locate': 24,\n",
       "  '_ed': 25,\n",
       "  'at': 26,\n",
       "  'point': 27,\n",
       "  'where': 28,\n",
       "  'lake': 29,\n",
       "  'empty': 30,\n",
       "  'into': 31,\n",
       "  'rhône': 32,\n",
       "  'river': 33,\n",
       "  '<STOP>': 34,\n",
       "  '<NUM>': 35,\n",
       "  '_': 36,\n",
       "  'year': 37,\n",
       "  'was': 38,\n",
       "  'leap': 39,\n",
       "  'start': 40,\n",
       "  '_ing': 41,\n",
       "  'on': 42,\n",
       "  'tuesday': 43,\n",
       "  'julian': 44,\n",
       "  'calendar': 45,\n",
       "  'event': 46,\n",
       "  'by': 47,\n",
       "  'place': 48,\n",
       "  'roman': 49,\n",
       "  'empire': 50,\n",
       "  'video': 51,\n",
       "  'game': 52,\n",
       "  'computer': 53,\n",
       "  'you': 54,\n",
       "  'can': 55,\n",
       "  'do': 56,\n",
       "  'thing': 57,\n",
       "  'that': 58,\n",
       "  'real': 59,\n",
       "  'life': 60,\n",
       "  'like': 61,\n",
       "  'make': 62,\n",
       "  'city': 63,\n",
       "  'these': 64,\n",
       "  'have': 65,\n",
       "  'wide': 66,\n",
       "  'variety': 67,\n",
       "  'topic': 68,\n",
       "  'but': 69,\n",
       "  'common': 70,\n",
       "  'are': 71,\n",
       "  'crime': 72,\n",
       "  'against': 73,\n",
       "  'humanity': 74,\n",
       "  'commit': 75,\n",
       "  'group': 76,\n",
       "  'people': 77,\n",
       "  'who': 78,\n",
       "  'not': 79,\n",
       "  'done': 80,\n",
       "  'anything': 81,\n",
       "  'wrong': 82,\n",
       "  'hurt': 83,\n",
       "  'just': 84,\n",
       "  'person': 85,\n",
       "  'or': 86,\n",
       "  'few': 87,\n",
       "  'they': 88,\n",
       "  'want': 89,\n",
       "  'to': 90,\n",
       "  'an': 91,\n",
       "  'entire': 92,\n",
       "  'for': 93,\n",
       "  'example': 94,\n",
       "  'nazi': 95,\n",
       "  'germany': 96,\n",
       "  'during': 97,\n",
       "  'holocaust': 98,\n",
       "  'nazis': 99,\n",
       "  'try': 100,\n",
       "  'kill': 101,\n",
       "  'all': 102,\n",
       "  'jewish': 103,\n",
       "  'europe': 104,\n",
       "  'this': 105,\n",
       "  'unlike': 106,\n",
       "  'war': 107,\n",
       "  'happen': 108,\n",
       "  'peace': 109,\n",
       "  'definition': 110,\n",
       "  'every': 111,\n",
       "  'big': 112,\n",
       "  'be': 113,\n",
       "  'must': 114,\n",
       "  'classical': 115,\n",
       "  'style': 116,\n",
       "  'music': 117,\n",
       "  'perform': 118,\n",
       "  'popular': 119,\n",
       "  'modern': 120,\n",
       "  'different': 121,\n",
       "  'how': 122,\n",
       "  'originally': 123,\n",
       "  'also': 124,\n",
       "  'refer': 125,\n",
       "  'performer': 126,\n",
       "  'normally': 127,\n",
       "  'play': 128,\n",
       "  'rock': 129,\n",
       "  'other': 130,\n",
       "  'adapt': 131,\n",
       "  'their': 132,\n",
       "  'there': 133,\n",
       "  'take': 134,\n",
       "  'dance': 135,\n",
       "  'band': 136,\n",
       "  'call': 137,\n",
       "  '\"': 138,\n",
       "  'made': 139,\n",
       "  'result': 140,\n",
       "  'republican': 141,\n",
       "  'party': 142,\n",
       "  'presidential': 143,\n",
       "  'primary': 144,\n",
       "  'article': 145,\n",
       "  'show': 146,\n",
       "  'process': 147,\n",
       "  'choose': 148,\n",
       "  'delegate': 149,\n",
       "  'represent': 150,\n",
       "  'someone': 151,\n",
       "  'go': 152,\n",
       "  'national': 153,\n",
       "  'convention': 154,\n",
       "  'each': 155,\n",
       "  'state': 156,\n",
       "  'will': 157,\n",
       "  'vote': 158,\n",
       "  'think': 159,\n",
       "  'good': 160,\n",
       "  'presidency': 161,\n",
       "  'candidate': 162,\n",
       "  'get': 163,\n",
       "  'certain': 164,\n",
       "  'amount': 165,\n",
       "  'base': 166,\n",
       "  'number': 167,\n",
       "  'receive': 168,\n",
       "  'began': 169,\n",
       "  'february': 170,\n",
       "  'iowa': 171,\n",
       "  'may': 172,\n",
       "  'donald': 173,\n",
       "  'trump': 174,\n",
       "  'became': 175,\n",
       "  'only': 176,\n",
       "  'left': 177,\n",
       "  'race': 178,\n",
       "  'he': 179,\n",
       "  'official': 180,\n",
       "  'nominee': 181,\n",
       "  'july': 182,\n",
       "  'would': 183,\n",
       "  'later': 184,\n",
       "  'win': 185,\n",
       "  'november': 186,\n",
       "  'main': 187,\n",
       "  'follow': 188,\n",
       "  'table': 189,\n",
       "  '/': 190,\n",
       "  'had': 191,\n",
       "  'major': 192,\n",
       "  'campaign': 193,\n",
       "  'legend': 194,\n",
       "  'stadium': 195,\n",
       "  'sport': 196,\n",
       "  'south': 197,\n",
       "  'africa': 198,\n",
       "  'fifa': 199,\n",
       "  'cup': 200,\n",
       "  'host': 201,\n",
       "  'games': 202,\n",
       "  'match': 203,\n",
       "  'time': 204,\n",
       "  'given': 205,\n",
       "  'african': 206,\n",
       "  'standard': 207,\n",
       "  'white': 208,\n",
       "  'county': 209,\n",
       "  'indiana': 210,\n",
       "  'u': 211,\n",
       "  's': 212,\n",
       "  'as': 213,\n",
       "  'live': 214,\n",
       "  'seat': 215,\n",
       "  'tape': 216,\n",
       "  'drive': 217,\n",
       "  'component': 218,\n",
       "  'often': 219,\n",
       "  'attach': 220,\n",
       "  'use': 221,\n",
       "  'write': 222,\n",
       "  'data': 223,\n",
       "  'magnetic': 224,\n",
       "  'read': 225,\n",
       "  'from': 226,\n",
       "  'usually': 227,\n",
       "  'hard': 228,\n",
       "  'access': 229,\n",
       "  'mean': 230,\n",
       "  'part': 231,\n",
       "  'before': 232,\n",
       "  'need': 233,\n",
       "  'well': 234,\n",
       "  'slow': 235,\n",
       "  'compare': 236,\n",
       "  'current': 237,\n",
       "  'capacity': 238,\n",
       "  'range': 239,\n",
       "  'ten': 240,\n",
       "  'almost': 241,\n",
       "  'per': 242,\n",
       "  'up': 243,\n",
       "  '-': 244,\n",
       "  'fifth': 245,\n",
       "  'studio': 246,\n",
       "  'album': 247,\n",
       "  'american': 248,\n",
       "  'singer': 249,\n",
       "  'songwriter': 250,\n",
       "  'wonder': 251,\n",
       "  'release': 252,\n",
       "  'reach': 253,\n",
       "  'no': 254,\n",
       "  'billboard': 255,\n",
       "  'pop': 256,\n",
       "  'chart': 257,\n",
       "  'r': 258,\n",
       "  '&': 259,\n",
       "  'b': 260,\n",
       "  'albums': 261,\n",
       "  'track': 262,\n",
       "  'listing': 263,\n",
       "  '#': 264,\n",
       "  'love': 265,\n",
       "  'hold': 266,\n",
       "  'me': 267,\n",
       "  'wind': 268,\n",
       "  '<': 269,\n",
       "  'br': 270,\n",
       "  '>': 271,\n",
       "  'nothing': 272,\n",
       "  'too': 273,\n",
       "  'my': 274,\n",
       "  'baby': 275,\n",
       "  'teach': 276,\n",
       "  'ai': 277,\n",
       "  'nt': 278,\n",
       "  'ask': 279,\n",
       "  'trouble': 280,\n",
       "  'i': 281,\n",
       "  'back': 282,\n",
       "  'pretty': 283,\n",
       "  'little': 284,\n",
       "  'angel': 285,\n",
       "  'talk': 286,\n",
       "  'contract': 287,\n",
       "  'with': 288,\n",
       "  'heart': 289,\n",
       "  'union': 290,\n",
       "  'new': 291,\n",
       "  'mexico': 292,\n",
       "  'census': 293,\n",
       "  'population': 294,\n",
       "  'fourth': 295,\n",
       "  'least': 296,\n",
       "  'populous': 297,\n",
       "  '-PRON-': 298,\n",
       "  'form': 299,\n",
       "  'english': 300,\n",
       "  'sometimes': 301,\n",
       "  'england': 302,\n",
       "  'which': 303,\n",
       "  'nation': 304,\n",
       "  'island': 305,\n",
       "  'great': 306,\n",
       "  'britain': 307,\n",
       "  'share': 308,\n",
       "  'scottish': 309,\n",
       "  'welsh': 310,\n",
       "  'were': 311,\n",
       "  'descend': 312,\n",
       "  'anglo': 313,\n",
       "  'saxon': 314,\n",
       "  'nowadays': 315,\n",
       "  'them': 316,\n",
       "  'another': 317,\n",
       "  'european': 318,\n",
       "  'country': 319,\n",
       "  'rest': 320,\n",
       "  'after': 321,\n",
       "  'uk': 322,\n",
       "  'wave': 323,\n",
       "  'immigrant': 324,\n",
       "  'many': 325,\n",
       "  'leave': 326,\n",
       "  'former': 327,\n",
       "  'british': 328,\n",
       "  'colony': 329,\n",
       "  'such': 330,\n",
       "  'usa': 331,\n",
       "  'australia': 332,\n",
       "  'zealand': 333,\n",
       "  'canada': 334,\n",
       "  'belize': 335,\n",
       "  'united': 336,\n",
       "  'states': 337,\n",
       "  'senate': 338,\n",
       "  'committee': 339,\n",
       "  'environment': 340,\n",
       "  'public': 341,\n",
       "  'work': 342,\n",
       "  'responsible': 343,\n",
       "  'deal': 344,\n",
       "  'issue': 345,\n",
       "  'relate': 346,\n",
       "  'natural': 347,\n",
       "  'member': 348,\n",
       "  'first': 349,\n",
       "  'letter': 350,\n",
       "  'alphabet': 351,\n",
       "  'came': 352,\n",
       "  'greek': 353,\n",
       "  'alpha': 354,\n",
       "  'long': 355,\n",
       "  'ago': 356,\n",
       "  'stood': 357,\n",
       "  'look': 358,\n",
       "  'exactly': 359,\n",
       "  'same': 360,\n",
       "  'language': 361,\n",
       "  'say': 362,\n",
       "  '[': 363,\n",
       "  ']': 364,\n",
       "  'bass': 365,\n",
       "  'word': 366,\n",
       "  'see': 367,\n",
       "  'name': 368,\n",
       "  'some': 369,\n",
       "  'specie': 370,\n",
       "  'fish': 371,\n",
       "  'caught': 372,\n",
       "  'food': 373,\n",
       "  'freshwater': 374,\n",
       "  'near': 375,\n",
       "  'north': 376,\n",
       "  'america': 377,\n",
       "  'order': 378,\n",
       "  'come': 379,\n",
       "  'known': 380,\n",
       "  'asia': 381,\n",
       "  'southern': 382,\n",
       "  'continent': 383,\n",
       "  'indian': 384,\n",
       "  'region': 385,\n",
       "  'surround': 386,\n",
       "  'west': 387,\n",
       "  'central': 388,\n",
       "  'eastern': 389,\n",
       "  'southeastern': 390,\n",
       "  'ocean': 391,\n",
       "  'mirror': 392,\n",
       "  'glass': 393,\n",
       "  'something': 394,\n",
       "  'reflect': 395,\n",
       "  'light': 396,\n",
       "  'plane': 397,\n",
       "  'piece': 398,\n",
       "  'special': 399,\n",
       "  'flat': 400,\n",
       "  'what': 401,\n",
       "  'behind': 402,\n",
       "  'metal': 403,\n",
       "  'surface': 404,\n",
       "  'water': 405,\n",
       "  'act': 406,\n",
       "  'reason': 407,\n",
       "  'able': 408,\n",
       "  'because': 409,\n",
       "  'your': 410,\n",
       "  'property': 411,\n",
       "  'image': 412,\n",
       "  'appear': 413,\n",
       "  'smaller': 414,\n",
       "  'larger': 415,\n",
       "  'than': 416,\n",
       "  'actual': 417,\n",
       "  'size': 418,\n",
       "  'angle': 419,\n",
       "  'view': 420,\n",
       "  'respectively': 421,\n",
       "  'appearance': 422,\n",
       "  'quite': 423,\n",
       "  'strange': 424,\n",
       "  'writing': 425,\n",
       "  'original': 426,\n",
       "  'out': 427,\n",
       "  'silver': 428,\n",
       "  'even': 429,\n",
       "  'if': 430,\n",
       "  'less': 431,\n",
       "  'curve': 432,\n",
       "  'bigger': 433,\n",
       "  'jerusalem': 434,\n",
       "  'establish': 435,\n",
       "  'swedish': 436,\n",
       "  'christian': 437,\n",
       "  'palestine': 438,\n",
       "  'occupy': 439,\n",
       "  'jordan': 440,\n",
       "  'until': 441,\n",
       "  'six': 442,\n",
       "  'day': 443,\n",
       "  'cemetery': 444,\n",
       "  'garden': 445,\n",
       "  'hebrew': 446,\n",
       "  'university': 447,\n",
       "  'mount': 448,\n",
       "  'detroit': 449,\n",
       "  'lion': 450,\n",
       "  'team': 451,\n",
       "  'football': 452,\n",
       "  'league': 453,\n",
       "  'conference': 454,\n",
       "  'michigan': 455,\n",
       "  'tiger': 456,\n",
       "  'home': 457,\n",
       "  'baseball': 458,\n",
       "  'club': 459,\n",
       "  'both': 460,\n",
       "  'cat': 461,\n",
       "  'consider': 462,\n",
       "  'dangerous': 463,\n",
       "  'madison': 464,\n",
       "  'nebraska': 465,\n",
       "  'pandemic': 466,\n",
       "  'covid': 467,\n",
       "  'confirm': 468,\n",
       "  'spread': 469,\n",
       "  'when': 470,\n",
       "  'case': 471,\n",
       "  'outbreak': 472,\n",
       "  'italy': 473,\n",
       "  'old': 474,\n",
       "  'man': 475,\n",
       "  'italian': 476,\n",
       "  'speak': 477,\n",
       "  'canton': 478,\n",
       "  'ticino': 479,\n",
       "  'border': 480,\n",
       "  'test': 481,\n",
       "  'positive': 482,\n",
       "  'previously': 483,\n",
       "  'visit': 484,\n",
       "  'milan': 485,\n",
       "  'afterwards': 486,\n",
       "  'multiple': 487,\n",
       "  'area': 488,\n",
       "  'discover': 489,\n",
       "  'include': 490,\n",
       "  'graubünden': 491,\n",
       "  'isolate': 492,\n",
       "  'cluster': 493,\n",
       "  'response': 494,\n",
       "  'government': 495,\n",
       "  'federal': 496,\n",
       "  'council': 497,\n",
       "  'ban': 498,\n",
       "  'more': 499,\n",
       "  'participant': 500,\n",
       "  'march': 501,\n",
       "  'school': 502,\n",
       "  'shop': 503,\n",
       "  'close': 504,\n",
       "  'five': 505,\n",
       "  'space': 506,\n",
       "  'put': 507,\n",
       "  'crossing': 508,\n",
       "  'announce': 509,\n",
       "  'economic': 510,\n",
       "  'support': 511,\n",
       "  'measure': 512,\n",
       "  'worth': 513,\n",
       "  'billion': 514,\n",
       "  'swiss': 515,\n",
       "  'bob': 516,\n",
       "  'robert': 517,\n",
       "  'earl': 518,\n",
       "  'retire': 519,\n",
       "  'professional': 520,\n",
       "  'basketball': 521,\n",
       "  'player': 522,\n",
       "  'join': 523,\n",
       "  'chicago': 524,\n",
       "  'bull': 525,\n",
       "  'association': 526,\n",
       "  'now': 527,\n",
       "  'director': 528,\n",
       "  'community': 529,\n",
       "  'affair': 530,\n",
       "  'keith': 531,\n",
       "  'judge': 532,\n",
       "  'senior': 533,\n",
       "  'court': 534,\n",
       "  'appeal': 535,\n",
       "  'sixth': 536,\n",
       "  'circuit': 537,\n",
       "  'serve': 538,\n",
       "  'retirement': 539,\n",
       "  'nominate': 540,\n",
       "  'president': 541,\n",
       "  'jimmy': 542,\n",
       "  'carter': 543,\n",
       "  'born': 544,\n",
       "  'study': 545,\n",
       "  'wayne': 546,\n",
       "  'howard': 547,\n",
       "  'virginia': 548,\n",
       "  'marry': 549,\n",
       "  'rachel': 550,\n",
       "  'three': 551,\n",
       "  'daughter': 552,\n",
       "  'die': 553,\n",
       "  'april': 554,\n",
       "  'age': 555,\n",
       "  'cause': 556,\n",
       "  'problem': 557,\n",
       "  'disease': 558,\n",
       "  'industrial': 559,\n",
       "  'constituency': 560,\n",
       "  'election': 561,\n",
       "  'legislative': 562,\n",
       "  'hong': 563,\n",
       "  'kong': 564,\n",
       "  'create': 565,\n",
       "  'compose': 566,\n",
       "  'industry': 567,\n",
       "  'theory': 568,\n",
       "  'branch': 569,\n",
       "  'mathematic': 570,\n",
       "  'generally': 571,\n",
       "  'seen': 572,\n",
       "  'belong': 573,\n",
       "  'science': 574,\n",
       "  'field': 575,\n",
       "  'subject': 576,\n",
       "  'solve': 577,\n",
       "  'then': 578,\n",
       "  'question': 579,\n",
       "  'know': 580,\n",
       "  'way': 581,\n",
       "  'two': 582,\n",
       "  'since': 583,\n",
       "  'theoretical': 584,\n",
       "  'replace': 585,\n",
       "  'system': 586,\n",
       "  'machine': 587,\n",
       "  'once': 588,\n",
       "  'solution': 589,\n",
       "  'exist': 590,\n",
       "  'scientist': 591,\n",
       "  'found': 592,\n",
       "  'perhaps': 593,\n",
       "  'improve': 594,\n",
       "  'cover': 595,\n",
       "  'develop': 596,\n",
       "  'concept': 597,\n",
       "  'method': 598,\n",
       "  'algorithm': 599,\n",
       "  'similar': 600,\n",
       "  'cooking': 601,\n",
       "  'recipe': 602,\n",
       "  'easier': 603,\n",
       "  'assault': 604,\n",
       "  'bridge': 605,\n",
       "  'built': 606,\n",
       "  'military': 607,\n",
       "  'heavy': 608,\n",
       "  'truck': 609,\n",
       "  'france': 610,\n",
       "  'french': 611,\n",
       "  'army': 612,\n",
       "  'purpose': 613,\n",
       "  'mobile': 614,\n",
       "  'vehicle': 615,\n",
       "  'town': 616,\n",
       "  'edinburgh': 617,\n",
       "  'scotland': 618,\n",
       "  'together': 619,\n",
       "  'unesco': 620,\n",
       "  'heritage': 621,\n",
       "  'site': 622,\n",
       "  'stage': 623,\n",
       "  'between': 624,\n",
       "  'around': 625,\n",
       "  'kept': 626,\n",
       "  'much': 627,\n",
       "  'period': 628,\n",
       "  'architecture': 629,\n",
       "  'prince': 630,\n",
       "  'street': 631,\n",
       "  'face': 632,\n",
       "  'castle': 633,\n",
       "  'jackson': 634,\n",
       "  'wisconsin': 635,\n",
       "  'black': 636,\n",
       "  'fall': 637,\n",
       "  'st': 638,\n",
       "  'john': 639,\n",
       "  'actor': 640,\n",
       "  'role': 641,\n",
       "  'neil': 642,\n",
       "  'winter': 643,\n",
       "  'young': 644,\n",
       "  'eleven': 645,\n",
       "  'emmy': 646,\n",
       "  'award': 647,\n",
       "  'nomination': 648,\n",
       "  'york': 649,\n",
       "  'dead': 650,\n",
       "  'los': 651,\n",
       "  'angeles': 652,\n",
       "  'california': 653,\n",
       "  'aged': 654,\n",
       "  'death': 655,\n",
       "  'been': 656,\n",
       "  'rule': 657,\n",
       "  'list': 658,\n",
       "  'failure': 659,\n",
       "  'legacy': 660,\n",
       "  'king': 661,\n",
       "  'second': 662,\n",
       "  'september': 663,\n",
       "  'let': 664,\n",
       "  'hammer': 665,\n",
       "  'remember': 666,\n",
       "  'end': 667,\n",
       "  'rainbow': 668,\n",
       "  'stronger': 669,\n",
       "  'warrior': 670,\n",
       "  'faith': 671,\n",
       "  'fallen': 672,\n",
       "  'personnel': 673,\n",
       "  'western': 674,\n",
       "  'christianity': 675,\n",
       "  'religion': 676,\n",
       "  'about': 677,\n",
       "  'religious': 678,\n",
       "  'tradition': 679,\n",
       "  'whose': 680,\n",
       "  'ancestor': 681,\n",
       "  'catholic': 682,\n",
       "  'church': 683,\n",
       "  'protestant': 684,\n",
       "  'leader': 685,\n",
       "  'belief': 686,\n",
       "  'catholics': 687,\n",
       "  'however': 688,\n",
       "  'set': 689,\n",
       "  '%': 690,\n",
       "  'christians': 691,\n",
       "  'worldwide': 692,\n",
       "  'over': 693,\n",
       "  'half': 694,\n",
       "  'related': 695,\n",
       "  'baptist': 696,\n",
       "  'anglican': 697,\n",
       "  'older': 698,\n",
       "  'third': 699,\n",
       "  'promote': 700,\n",
       "  'hit': 701,\n",
       "  'song': 702,\n",
       "  'bottom': 703,\n",
       "  'very': 704,\n",
       "  'previous': 705,\n",
       "  'care': 706,\n",
       "  'got': 707,\n",
       "  'news': 708,\n",
       "  'comedy': 709,\n",
       "  'television': 710,\n",
       "  'panel': 711,\n",
       "  'shown': 712,\n",
       "  'bbc': 713,\n",
       "  'episode': 714,\n",
       "  'compete': 715,\n",
       "  'answer': 716,\n",
       "  'recent': 717,\n",
       "  'captain': 718,\n",
       "  'celebrity': 719,\n",
       "  'guest': 720,\n",
       "  'ian': 721,\n",
       "  'paul': 722,\n",
       "  'october': 723,\n",
       "  'presenter': 724,\n",
       "  'series': 725,\n",
       "  'programme': 726,\n",
       "  'bank': 727,\n",
       "  'land': 728,\n",
       "  'middle': 729,\n",
       "  'east': 730,\n",
       "  'touch': 731,\n",
       "  'sea': 732,\n",
       "  'landscape': 733,\n",
       "  'mostly': 734,\n",
       "  'hill': 735,\n",
       "  'desert': 736,\n",
       "  'weather': 737,\n",
       "  'summer': 738,\n",
       "  'hot': 739,\n",
       "  'dry': 740,\n",
       "  'right': 741,\n",
       "  'under': 742,\n",
       "  'control': 743,\n",
       "  'israel': 744,\n",
       "  'disagree': 745,\n",
       "  'really': 746,\n",
       "  'israeli': 747,\n",
       "  'strip': 748,\n",
       "  'future': 749,\n",
       "  'settler': 750,\n",
       "  'settlement': 751,\n",
       "  'controversy': 752,\n",
       "  'claim': 753,\n",
       "  'kingdom': 754,\n",
       "  'northern': 755,\n",
       "  'ireland': 756,\n",
       "  'simply': 757,\n",
       "  'sovereign': 758,\n",
       "  'constitutional': 759,\n",
       "  'monarchy': 760,\n",
       "  'four': 761,\n",
       "  'separate': 762,\n",
       "  'wales': 763,\n",
       "  'nations': 764,\n",
       "  'commonwealth': 765,\n",
       "  'nato': 766,\n",
       "  'economy': 767,\n",
       "  'million': 768,\n",
       "  'divide': 769,\n",
       "  'eighth': 770,\n",
       "  'alternative': 771,\n",
       "  'red': 772,\n",
       "  'pepper': 773,\n",
       "  'record': 774,\n",
       "  'label': 775,\n",
       "  'warner': 776,\n",
       "  'bros': 777,\n",
       "  'records': 778,\n",
       "  'sold': 779,\n",
       "  'copy': 780,\n",
       "  'week': 781,\n",
       "  'position': 782,\n",
       "  'single': 783,\n",
       "  'ca': 784,\n",
       "  'stop': 785,\n",
       "  'lyric': 786,\n",
       "  'past': 787,\n",
       "  'critic': 788,\n",
       "  'thought': 789,\n",
       "  'bit': 790,\n",
       "  'quark': 791,\n",
       "  'particle': 792,\n",
       "  'help': 793,\n",
       "  'proton': 794,\n",
       "  'charge': 795,\n",
       "  '+': 796,\n",
       "  'type': 797,\n",
       "  'spin': 798,\n",
       "  'affect': 799,\n",
       "  'fundamental': 800,\n",
       "  'force': 801,\n",
       "  'gravity': 802,\n",
       "  'strong': 803,\n",
       "  'weak': 804,\n",
       "  'elementary': 805,\n",
       "  'so': 806,\n",
       "  'small': 807,\n",
       "  'believe': 808,\n",
       "  'any': 809,\n",
       "  'down': 810,\n",
       "  'neutron': 811,\n",
       "  'complex': 812,\n",
       "  'berlin': 813,\n",
       "  'greater': 814,\n",
       "  'prussian': 815,\n",
       "  'parliament': 816,\n",
       "  'short': 817,\n",
       "  'full': 818,\n",
       "  'law': 819,\n",
       "  'regard': 820,\n",
       "  'local': 821,\n",
       "  'authority': 822,\n",
       "  'said': 823,\n",
       "  'should': 824,\n",
       "  'taken': 825,\n",
       "  'away': 826,\n",
       "  'province': 827,\n",
       "  'become': 828,\n",
       "  'district': 829,\n",
       "  'attorney': 830,\n",
       "  'general': 831,\n",
       "  'legal': 832,\n",
       "  'advisor': 833,\n",
       "  'she': 834,\n",
       "  'might': 835,\n",
       "  'officer': 836,\n",
       "  'crown': 837,\n",
       "  'gang': 838,\n",
       "  'norse': 839,\n",
       "  'men': 840,\n",
       "  'criminal': 841,\n",
       "  'several': 842,\n",
       "  'why': 843,\n",
       "  'lot': 844,\n",
       "  'chance': 845,\n",
       "  'money': 846,\n",
       "  'through': 847,\n",
       "  'sell': 848,\n",
       "  'illegal': 849,\n",
       "  'drug': 850,\n",
       "  'especially': 851,\n",
       "  'younger': 852,\n",
       "  'tell': 853,\n",
       "  'territory': 854,\n",
       "  'stay': 855,\n",
       "  'send': 856,\n",
       "  'message': 857,\n",
       "  'dress': 858,\n",
       "  'violence': 859,\n",
       "  'innocent': 860,\n",
       "  'extremely': 861,\n",
       "  'across': 862,\n",
       "  'moscow': 863,\n",
       "  'nearly': 864,\n",
       "  'history': 865,\n",
       "  'june': 866,\n",
       "  'note': 867,\n",
       "  'month': 868,\n",
       "  'please': 869,\n",
       "  'date': 870,\n",
       "  'last': 871,\n",
       "  'non': 872,\n",
       "  'human': 873,\n",
       "  'here': 874,\n",
       "  'source': 875,\n",
       "  'reference': 876,\n",
       "  'notice': 877,\n",
       "  'remove': 878,\n",
       "  'requirement': 879,\n",
       "  'add': 880,\n",
       "  'rhode': 881,\n",
       "  'smallest': 882,\n",
       "  'capital': 883,\n",
       "  'bay': 884,\n",
       "  'thirteen': 885,\n",
       "  'massachusetts': 886,\n",
       "  'connecticut': 887,\n",
       "  'sound': 888,\n",
       "  'atlantic': 889,\n",
       "  'roger': 890,\n",
       "  'williams': 891,\n",
       "  'freedom': 892,\n",
       "  'greece': 893,\n",
       "  'executive': 894,\n",
       "  'sign': 895,\n",
       "  'governor': 896,\n",
       "  'plantation': 897,\n",
       "  'due': 898,\n",
       "  'tie': 899,\n",
       "  'slavery': 900,\n",
       "  'change': 901,\n",
       "  'effect': 902,\n",
       "  'soon': 903,\n",
       "  'tommy': 904,\n",
       "  'thomas': 905,\n",
       "  'footballer': 906,\n",
       "  'jefferson': 907,\n",
       "  'sitcom': 908,\n",
       "  'star': 909,\n",
       "  'sherman': 910,\n",
       "  'couple': 911,\n",
       "  'broadcast': 912,\n",
       "  't': 913,\n",
       "  'communication': 914,\n",
       "  'company': 915,\n",
       "  'january': 916,\n",
       "  'longest': 917,\n",
       "  'run': 918,\n",
       "  'kentucky': 919,\n",
       "  'dennis': 920,\n",
       "  'blair': 921,\n",
       "  'intelligence': 922,\n",
       "  'navy': 923,\n",
       "  'admiral': 924,\n",
       "  'commander': 925,\n",
       "  'pacific': 926,\n",
       "  'career': 927,\n",
       "  'house': 928,\n",
       "  'ronald': 929,\n",
       "  'reagan': 930,\n",
       "  'select': 931,\n",
       "  'barack': 932,\n",
       "  'obama': 933,\n",
       "  '’s': 934,\n",
       "  'battle': 935,\n",
       "  'resign': 936,\n",
       "  'xbox': 937,\n",
       "  'console': 938,\n",
       "  'microsoft': 939,\n",
       "  'employee': 940,\n",
       "  'decide': 941,\n",
       "  'playstation': 942,\n",
       "  'took': 943,\n",
       "  'apart': 944,\n",
       "  'box': 945,\n",
       "  'graphic': 946,\n",
       "  'program': 947,\n",
       "  'ed': 948,\n",
       "  'fry': 949,\n",
       "  'publishing': 950,\n",
       "  'business': 951,\n",
       "  'idea': 952,\n",
       "  'project': 953,\n",
       "  'generation': 954,\n",
       "  'nba': 955,\n",
       "  'jam': 956,\n",
       "  'publish': 957,\n",
       "  'gameplay': 958,\n",
       "  'arch': 959,\n",
       "  'rival': 960,\n",
       "  'culture': 961,\n",
       "  'enter': 962,\n",
       "  'specific': 963,\n",
       "  'code': 964,\n",
       "  'bill': 965,\n",
       "  'clinton': 966,\n",
       "  'malta': 967,\n",
       "  'summit': 968,\n",
       "  'comprise': 969,\n",
       "  'meeting': 970,\n",
       "  'george': 971,\n",
       "  'h': 972,\n",
       "  'w': 973,\n",
       "  'bush': 974,\n",
       "  'december': 975,\n",
       "  'wall': 976,\n",
       "  'declare': 977,\n",
       "  'cold': 978,\n",
       "  'although': 979,\n",
       "  'whether': 980,\n",
       "  'matter': 981,\n",
       "  'debate': 982,\n",
       "  'won': 983,\n",
       "  'uefa': 984,\n",
       "  'championship': 985,\n",
       "  ';': 986,\n",
       "  'j': 987,\n",
       "  'jazz': 988,\n",
       "  'guitarist': 989,\n",
       "  'producer': 990,\n",
       "  'best': 991,\n",
       "  'kiss': 992,\n",
       "  'p': 993,\n",
       "  'las': 994,\n",
       "  'vegas': 995,\n",
       "  'nevada': 996,\n",
       "  'charles': 997,\n",
       "  'sweden': 998,\n",
       "  'expand': 999,\n",
       "  ...})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data(\"train.txt\", \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "DSdbu7ma3a-6",
    "outputId": "3efb6bc0-c835-472b-9d64-44930cf2ef61"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-bfb78887f8cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-bfb78887f8cc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;31m# TODO: Set-up the training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-bfb78887f8cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_inputs, train_labels)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m       \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1739\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1740\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1742\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6013\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   6014\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6015\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   6016\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6017\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "import os\n",
    "\n",
    "# ensures that we run only on cpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        The Model class predicts the next words in a sequence.\n",
    "\n",
    "        :param vocab_size: The number of unique words in the data\n",
    "        \"\"\"\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # TODO: initialize emnbedding_size, batch_size, and any other hyperparameters\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = 30  # TODO\n",
    "        self.batch_size = 300 # TODO\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(1e-3) # learning rate is 1e-3\n",
    "        self.loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "\n",
    "        # TODO: initialize embeddings and forward pass weights (weights, biases)\n",
    "        def make_vars(*dims, initializer=tf.random.normal):\n",
    "          return tf.Variable(initializer(dims, stddev=.1))\n",
    "        \n",
    "        self.E = make_vars(self.vocab_size, self.embedding_size)   # E means embedding matrix \n",
    "        self.W1 = make_vars(2 * self.embedding_size, self.vocab_size) # because trigram has two inputs \n",
    "        self.b1 = make_vars(self.vocab_size)\n",
    "        self.W2 = make_vars(self.vocab_size, self.vocab_size)   # weight\n",
    "        self.b2 = make_vars(self.vocab_size)   # bias\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        You must use an embedding layer as the first layer of your network\n",
    "        (i.e. tf.nn.embedding_lookup)\n",
    "\n",
    "        :param inputs: word ids of shape (batch_size, 2)\n",
    "        :return: probabilities: The batch element probabilities as a tensor of shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Fill in\n",
    "        embed_lookup_0 = tf.nn.embedding_lookup(self.E, inputs[:, 0])  # (inputs[:, 0], embed_size)\n",
    "        embed_lookup_1 = tf.nn.embedding_lookup(self.E, inputs[:, 1])  # (inputs[:, 1], embed_size)\n",
    "        embed_inputs = tf.concat([embed_lookup_0, embed_lookup_1], 1) # (inputs, 2 * embed_size)\n",
    "        dense1 = tf.matmul(embed_inputs, self.W1) + self.b1\n",
    "        # (inputs, 2*embed_size) * (2*embed_size, vocab_size) + vocab_size\n",
    "        dense1_relu = tf.nn.relu(dense1)\n",
    "        dense2 = tf.matmul(dense1_relu, self.W2) + self.b2\n",
    "        # (inputs, vocab_size) * (vocab_size, vocab_size) + vocab_size\n",
    "\n",
    "        prob = tf.nn.softmax(dense2)\n",
    "\n",
    "        return prob\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def loss_function(self, probabilities, labels):\n",
    "        \"\"\"\n",
    "        Calculates average cross entropy sequence to sequence loss of the prediction\n",
    "\n",
    "        :param probabilities: a matrix of shape (batch_size, vocab_size)\n",
    "        :return: the average loss of the model as a tensor of size 1\n",
    "        \"\"\"\n",
    "        # TODO: Fill in\n",
    "        # We recommend using tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "        return tf.reduce_mean(self.loss(labels, probabilities))  # syntax: y_true, y_predict \n",
    "\n",
    "\n",
    "def train(model, train_inputs, train_labels):\n",
    "    \"\"\"\n",
    "    Runs through one epoch - all training examples.\n",
    "    Remember to shuffle your inputs and labels - ensure that they are shuffled\n",
    "    in the same order. Also you should batch your input and labels here.\n",
    "\n",
    "    :param model: the initilized model to use for forward and backward pass\n",
    "    :param train_input: train inputs (all inputs for training) of shape (num_inputs,2)\n",
    "    :param train_input: train labels (all labels for training) of shape (num_inputs,)\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO Fill in\n",
    "    \n",
    "    # shuffle the inputs and labels\n",
    "    random_index = tf.random.shuffle(np.array(range(train_inputs.shape[0])))\n",
    "    train_inputs = tf.gather(train_inputs, random_index)\n",
    "    train_labels = tf.gather(train_labels, random_index)\n",
    "\n",
    "\n",
    "\n",
    "    # use batch_size to train the model \n",
    "    train_steps = int(train_inputs.shape[0] / model.batch_size)\n",
    "\n",
    "    for i in range(train_steps):\n",
    "      start = i * model.batch_size\n",
    "      end = (i + 1) * model.batch_size\n",
    "      if (i + 1) * model.batch_size > train_inputs.shape[0]:\n",
    "        end = train_inputs.shape[0]\n",
    "      X = train_inputs[start:end]\n",
    "      Y = train_labels[start:end]\n",
    "      \n",
    "      with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = model.loss_function(y_pred, Y)\n",
    "\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "      model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_inputs, test_labels):\n",
    "    \"\"\"\n",
    "    Runs through all test examples. Test input should be batched here.\n",
    "\n",
    "    :param model: the trained model to use for prediction\n",
    "    :param test_input: train inputs (all inputs for testing) of shape (num_inputs,2)\n",
    "    :param test_input: train labels (all labels for testing) of shape (num_inputs,)\n",
    "    :returns: perplexity of the test set\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Fill in\n",
    "    # NOTE: Ensure a correct perplexity formula (different from raw loss)\n",
    "\n",
    "    train_steps = int(test_inputs.shape[0] / model.batch_size)\n",
    "\n",
    "    total_loss = []\n",
    "    for i in range(train_steps):\n",
    "      start = i * model.batch_size\n",
    "      end = (i + 1) * model.batch_size\n",
    "      if (i + 1) * model.batch_size > test_inputs.shape[0]:\n",
    "        end = test_inputs.shape[0]\n",
    "      X = test_inputs[start:end]\n",
    "      Y = test_labels[start:end]\n",
    "\n",
    "      # average cross entropy loss for 1 batch (similar to 1 sentence)\n",
    "      loss = tf.reduce_mean(model.loss_function(model(X), Y))\n",
    "      total_loss.append(loss)\n",
    "\n",
    "    perplexity = np.exp(np.mean(total_loss))\n",
    "\n",
    "    return perplexity\n",
    "      \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def generate_sentence(word1, word2, length, vocab, model):\n",
    "    \"\"\"\n",
    "    Given initial 2 words, print out predicted sentence of targeted length.\n",
    "\n",
    "    :param word1: string, first word\n",
    "    :param word2: string, second word\n",
    "    :param length: int, desired sentence length\n",
    "    :param vocab: dictionary, word to id mapping\n",
    "    :param model: trained trigram model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: This is a deterministic, argmax sentence generation\n",
    "\n",
    "    reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "    output_string = np.zeros((1, length), dtype=np.int)\n",
    "    output_string[:, :2] = vocab[word1], vocab[word2]\n",
    "\n",
    "    for end in range(2, length):\n",
    "        start = end - 2\n",
    "        output_string[:, end] = np.argmax(model(output_string[:, start:end]), axis=1)\n",
    "    text = [reverse_vocab[i] for i in list(output_string[0])]\n",
    "\n",
    "    print(\" \".join(text))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # TODO: Pre-process and vectorize the data using get_data from preprocess\n",
    "    train2id, test2id, word2id = get_data(\"train.txt\", \"test.txt\")\n",
    "\n",
    "    # TO-DO:  Separate your train and test data into inputs and labels\n",
    "    first_word_train = np.array(train2id[0:len(train2id)-2]).reshape(-1,1)\n",
    "    second_word_train = np.array(train2id[1:len(train2id)-1]).reshape(-1,1)\n",
    "    inputs_train = np.concatenate((first_word_train, second_word_train), 1)\n",
    "    labels_train = train2id[2:len(train2id)]\n",
    "\n",
    "    first_word_test = np.array(test2id[0:len(test2id)-2]).reshape(-1,1)\n",
    "    second_word_test = np.array(test2id[1:len(test2id)-1]).reshape(-1,1)\n",
    "    inputs_test = np.concatenate((first_word_test, second_word_test), 1)\n",
    "    labels_test = test2id[2:len(test2id)]\n",
    "\n",
    "    # TODO: initialize model\n",
    "    trigram = Model(len(word2id))\n",
    "\n",
    "    # TODO: Set-up the training step\n",
    "    train(trigram, inputs_train, labels_train)\n",
    "\n",
    "\n",
    "    # TODO: Set up the testing steps\n",
    "    perplexity = test(trigram, inputs_test, labels_test)\n",
    "\n",
    "\n",
    "    # Print out perplexity\n",
    "    print(perplexity)\n",
    "\n",
    "    # BONUS: Try printing out sentences with different starting words\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ey287NJVQGKy",
    "outputId": "283989db-c829-4524-a748-c9930328f809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n",
      "this is the shape of emb_inputs: (100, 20, 30)\n",
      "this is the shape of whole_seq_output: (100, 20, 30)\n",
      "this is the shape of final_hidden_state: (100, 30)\n",
      "this is the shape of final_cell_state: (100, 30)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9455c6261a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-9455c6261a59>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# TODO: Set-up the training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;31m# TODO: Set up the testing steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-9455c6261a59>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_inputs, train_labels)\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-9455c6261a59>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, probabilities, labels)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# We recommend using tf.keras.losses.sparse_categorical_crossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/losses.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, axis)\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m   return backend.sparse_categorical_crossentropy(\n\u001b[0;32m-> 1863\u001b[0;31m       y_true, y_pred, from_logits=from_logits, axis=axis)\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   5204\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5205\u001b[0m     res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 5206\u001b[0;31m         labels=target, logits=output)\n\u001b[0m\u001b[1;32m   5207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5208\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mupdate_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput_rank\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits_v2\u001b[0;34m(labels, logits, name)\u001b[0m\n\u001b[1;32m   4424\u001b[0m   \"\"\"\n\u001b[1;32m   4425\u001b[0m   return sparse_softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 4426\u001b[0;31m       labels=labels, logits=logits, name=name)\n\u001b[0m\u001b[1;32m   4427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m   4357\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4358\u001b[0m       cost = _sparse_softmax_cross_entropy_with_rank_2_logits(\n\u001b[0;32m-> 4359\u001b[0;31m           precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   4360\u001b[0m       \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4361\u001b[0m       \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_static_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_sparse_softmax_cross_entropy_with_rank_2_logits\u001b[0;34m(logits, labels, name)\u001b[0m\n\u001b[1;32m   4236\u001b[0m     \u001b[0;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4237\u001b[0m     cost, _ = gen_nn_ops.sparse_softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 4238\u001b[0;31m         logits, labels, name=name)\n\u001b[0m\u001b[1;32m   4239\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m  11323\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11324\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m> 11325\u001b[0;31m         _ctx, \"SparseSoftmaxCrossEntropyWithLogits\", name, features, labels)\n\u001b[0m\u001b[1;32m  11326\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SparseSoftmaxCrossEntropyWithLogitsOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11327\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "import os\n",
    "\n",
    "# ensures that we run only on cpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        The Model class predicts the next words in a sequence.\n",
    "\n",
    "        :param vocab_size: The number of unique words in the data\n",
    "        \"\"\"\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # TODO: initialize embedding_size, batch_size, and any other hyperparameters\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = 20\n",
    "        self.embedding_size = 30  # TODO\n",
    "        self.batch_size = 100  # TODO\n",
    "\n",
    "        self.rnn_size = 30   ## dimension of inner cell such as hidden state, output state, and cell state for exactly one LSTM block \n",
    "\n",
    "        # TODO: initialize embeddings and forward pass weights (weights, biases)\n",
    "        # Note: You can now use tf.keras.layers!\n",
    "        # - use tf.keras.layers.Dense for feed forward layers\n",
    "        # - and use tf.keras.layers.GRU or tf.keras.layers.LSTM for your RNN\n",
    "\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "        self.l = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "        ## RNNs\n",
    "        self.LSTM = tf.keras.layers.LSTM(self.rnn_size, return_sequences=True, return_state=True)\n",
    "\n",
    "        # return_sequences = True: all hidden states (hidden state of all the time steps)\n",
    "        # return_state = True: last hidden state + last cell state\n",
    "        \n",
    "        # Architecture \n",
    "        self.emb = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size) \n",
    "        # The model will take as input an integer matrix of size (batch_size, window_size)\n",
    "        # and the largest integer (i.e. word index) in the input should be no larger than vocab_size\n",
    "\n",
    "        self.mlp1 = tf.keras.layers.Dense(100, activation = 'relu') \n",
    "        self.mlp2 = tf.keras.layers.Dense(self.vocab_size, activation = 'softmax')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, inputs, initial_state):\n",
    "        \"\"\"\n",
    "        - You must use an embedding layer as the first layer of your network\n",
    "        (i.e. tf.nn.embedding_lookup)\n",
    "        - You must use an LSTM or GRU as the next layer.\n",
    "\n",
    "        :param inputs: word ids of shape (batch_size, window_size)\n",
    "        :param initial_state: 2-d array of shape (batch_size, rnn_size) as a tensor\n",
    "        :return: the batch element probabilities as a tensor, a final_state\n",
    "        (NOTE 1: If you use an LSTM, the final_state will be the last two RNN outputs,\n",
    "        NOTE 2: We only need to use the initial state during generation)\n",
    "        using LSTM and only the probabilites as a tensor and a final_state as a tensor when using GRU\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Fill in\n",
    "        emb_inputs = self.emb(inputs) # embedding lookup\n",
    "                                      # (batch_size, window_size, embedding_size)\n",
    "\n",
    "        print(f'this is the shape of emb_inputs: {emb_inputs.shape}')  # (100, 20, 30)\n",
    "\n",
    "\n",
    "        whole_seq_output, final_hidden_state, final_cell_state = self.LSTM(emb_inputs, initial_state = initial_state)\n",
    "        # inputs should be: (batch_size, timesteps(window_size), feature(embedding_size))\n",
    "        # whole_seq_output: (batch_size, timesteps(window_size), unit/rnn_size)\n",
    "        # final_hidden_state, final_cell_state: (batch_size, unit/rnn_size): the final embedding for each batch_size element \n",
    "                                                                           # after going through the lstm block 20 times\n",
    "\n",
    "\n",
    "        # if the window_size is 20, the model can only model relationship within these 20 words\n",
    "        # there will be 20 lstm blocks, each time, it take one word as input\n",
    "        # basically trying to use first 19 words to predict the last word if the window_size is 20 \n",
    "\n",
    "\n",
    "        print(f'this is the shape of whole_seq_output: {whole_seq_output.shape}')   # (100, 20, 30)\n",
    "        print(f'this is the shape of final_hidden_state: {final_hidden_state.shape}')  # (100, 30)\n",
    "        print(f'this is the shape of final_cell_state: {final_cell_state.shape}')      # (100, 30)\n",
    "\n",
    "\n",
    "        dense = self.mlp1(whole_seq_output)\n",
    "        prob = self.mlp2(dense)\n",
    "\n",
    "        return prob, (final_hidden_state, final_cell_state)\n",
    "\n",
    "    def loss(self, probabilities, labels):\n",
    "        \"\"\"\n",
    "        Calculates average cross entropy sequence to sequence loss of the prediction\n",
    "\n",
    "        :param probabilities: a matrix of shape (batch_size, window_size, vocab_size) as a tensor\n",
    "        :param labels: matrix of shape (batch_size, window_size) containing the labels\n",
    "        :return: the average loss of the model as a tensor of size 1\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Fill in\n",
    "        # We recommend using tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "        return tf.reduce_mean(self.l(labels, probabilities))\n",
    "\n",
    "\n",
    "def train(model, train_inputs, train_labels):\n",
    "    \"\"\"\n",
    "    Runs through one epoch - all training examples (remember to batch!)\n",
    "    Here you will also want to reshape your inputs and labels so that they match\n",
    "    the inputs and labels shapes passed in the call and loss functions respectively.\n",
    "\n",
    "    :param model: the initilized model to use for forward and backward pass\n",
    "    :param train_inputs: train inputs (all inputs for training) of shape (num_inputs,)\n",
    "    :param train_labels: train labels (all labels for training) of shape (num_labels,)\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # TODO: Fill in\n",
    "\n",
    "\n",
    "    ## reshape\n",
    "\n",
    "    # print(f'this is train_inputs.shape: {np.array(train_inputs).shape}')  # (1465613,)\n",
    "    num = len(train_inputs) // model.window_size\n",
    "    train_inputs = np.array(train_inputs[0:num*model.window_size]).reshape(-1, model.window_size)\n",
    "    train_labels = np.array(train_labels[0:num*model.window_size]).reshape(-1, model.window_size)\n",
    "    # print(f'this is train_inputs.shape: {train_inputs.shape}')          # (73280, 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # shuffle the inputs and labels\n",
    "    random_index = tf.random.shuffle(np.array(range(train_inputs.shape[0])))\n",
    "    train_inputs = tf.gather(train_inputs, random_index)\n",
    "    train_labels = tf.gather(train_labels, random_index)\n",
    "\n",
    "\n",
    "\n",
    "    # use batch_size to train the model \n",
    "    train_steps = int(train_inputs.shape[0] / model.batch_size)\n",
    "\n",
    "    for i in range(train_steps):\n",
    "      start = i * model.batch_size\n",
    "      end = (i + 1) * model.batch_size\n",
    "      if (i + 1) * model.batch_size > train_inputs.shape[0]:\n",
    "        end = train_inputs.shape[0]\n",
    "      X = train_inputs[start:end]\n",
    "      Y = train_labels[start:end]\n",
    "      \n",
    "      with tf.GradientTape() as tape:\n",
    "        prob, final_state = model.call(X, initial_state = None)\n",
    "        total_loss = model.loss(prob, Y)\n",
    "      \n",
    "      gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "      model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "\n",
    "def test(model, test_inputs, test_labels):\n",
    "    \"\"\"\n",
    "    Runs through one epoch - all testing examples (remember to batch!)\n",
    "    Here you will also want to reshape your inputs and labels so that they match\n",
    "    the inputs and labels shapes passed in the call and loss functions respectively.\n",
    "\n",
    "    :param model: the trained model to use for prediction\n",
    "    :param test_inputs: train inputs (all inputs for testing) of shape (num_inputs,)\n",
    "    :param test_labels: train labels (all labels for testing) of shape (num_labels,)\n",
    "    :returns: perplexity of the test set\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Fill in\n",
    "    # NOTE: Ensure a correct perplexity formula (different from raw loss)\n",
    "\n",
    "\n",
    "    # reshape \n",
    "    num = len(test_inputs) // model.window_size\n",
    "    test_inputs = np.array(test_inputs[0:num*model.window_size]).reshape(-1, model.window_size)\n",
    "    test_labels = np.array(test_labels[0:num*model.window_size]).reshape(-1, model.window_size)\n",
    "\n",
    "\n",
    "    train_steps = int(test_inputs.shape[0] / model.batch_size)\n",
    "\n",
    "    \n",
    "    total_loss = []\n",
    "    for i in range(train_steps):\n",
    "      start = i * model.batch_size\n",
    "      end = (i + 1) * model.batch_size\n",
    "      if (i + 1) * model.batch_size > test_inputs.shape[0]:\n",
    "        end = test_inputs.shape[0]\n",
    "      X = test_inputs[start:end]\n",
    "      Y = test_labels[start:end]\n",
    "\n",
    "      # average cross entropy loss for 1 batch (similar to 1 sentence)\n",
    "      \n",
    "      prob, final_state = model.call(X, initial_state = None)\n",
    "      batch_loss = tf.reduce_mean(model.loss(prob, Y))\n",
    "      total_loss.append(batch_loss)\n",
    "\n",
    "    perplexity = np.exp(np.mean(total_loss))\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "def generate_sentence(word1, length, vocab, model, sample_n=10):\n",
    "    \"\"\"\n",
    "    Takes a model, vocab, selects from the most likely next word from the model's distribution\n",
    "\n",
    "    :param model: trained RNN model\n",
    "    :param vocab: dictionary, word to id mapping\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: Feel free to play around with different sample_n values\n",
    "\n",
    "    reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "    previous_state = None\n",
    "\n",
    "    first_string = word1\n",
    "    first_word_index = vocab[word1]\n",
    "    next_input = [[first_word_index]]\n",
    "    text = [first_string]\n",
    "\n",
    "    for i in range(length):\n",
    "        logits, previous_state = model.call(next_input, previous_state)\n",
    "        logits = np.array(logits[0, 0, :])\n",
    "        top_n = np.argsort(logits)[-sample_n:]\n",
    "        n_logits = np.exp(logits[top_n]) / np.exp(logits[top_n]).sum()\n",
    "        out_index = np.random.choice(top_n, p=n_logits)\n",
    "\n",
    "        text.append(reverse_vocab[out_index])\n",
    "        next_input = [[out_index]]\n",
    "\n",
    "    print(\" \".join(text))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # TODO: Pre-process and vectorize the data\n",
    "    # HINT: Please note that you are predicting the next word at each timestep,\n",
    "    # so you want to remove the last element from train_x and test_x.\n",
    "    # You also need to drop the first element from train_y and test_y.\n",
    "    # If you don't do this, you will see impossibly small perplexities.\n",
    "\n",
    "    # TODO: Separate your train and test data into inputs and labels\n",
    "    train_data, test_data, word2id = get_data(\"train.txt\", \"test.txt\")\n",
    "\n",
    "    inputs_train = train_data[0:len(train_data)-1]\n",
    "    labels_train = train_data[1:len(train_data)]\n",
    "    inputs_test = test_data[0:len(test_data)-1]\n",
    "    labels_test = test_data[1:len(test_data)]\n",
    "    vocab_size = len(word2id)\n",
    "\n",
    "    # TODO: initialize model and tensorflow variables\n",
    "    LSTM = Model(vocab_size)\n",
    "\n",
    "    # TODO: Set-up the training step\n",
    "    train(LSTM, inputs_train, labels_train)\n",
    "\n",
    "    # TODO: Set up the testing steps\n",
    "    perplexity = test(LSTM, inputs_test, labels_test)\n",
    "\n",
    "    # Print out perplexity\n",
    "    print(perplexity)\n",
    "\n",
    "    # BONUS: Try printing out various sentences with different start words and sample_n parameters\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSCI2470HW3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
