{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mu-QP0s0LlTI"
   },
   "source": [
    "# 1. Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-4Z2wR1LpU3"
   },
   "source": [
    "1. What is the purpose of the positional encoding in the Transformer architecture? What is the size of a positional encoding vector? (2-4 sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wgaAaaWL-X_"
   },
   "source": [
    "In the Transformer architecture, since we don't have the sequential relationship between inputs, positional encoding is used to give the order context to the non-recurrent architecture of Transformer. The size of positional encoding is embedding_size because we will add it to the embedding of input word to get embedding with time signal vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOGJC4IWM_EV"
   },
   "source": [
    "2. What are the limitations of RNNs that Transformers solve? (3-6 sen- tences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT7glA6ONSz8"
   },
   "source": [
    "First of all, RNN training is hard to parallelize since the previous word must be processed before next word becuase of the sequential structure. Second, even with LSTM and GRU, preserving important linguistic context over very long sequences is difficult. For LSTM and GRU, they mainly capture the relationship of words within the hyperparameter window_size. In contrast, Transformer doesn't have this recurrent/sequential structure and only uses attention. So the training for Transformer is parallelizable and it doesn't even try to remember things (every step looks at a weighted combination of all words in the input sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYbKVpjdPcOc"
   },
   "source": [
    "3. Consider the parameters for two different attention heads. Is it necessary that they be initialized randomly, or could we just start them all with the same vector of values? (2-4 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpc_U839Pmf2"
   },
   "source": [
    "It is necessary to randomly initialize attention heads. The reason we want to use multi-head attention instead of self attention is because we want each head to learn different relationships between words in the sentence. If we start them all with the same vector of values, each attention head may just learn the same relationship between the query word and the key words. Then it is meaningless to utilize multi-head attention; we can just use self attention instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP1rhotbck8s"
   },
   "source": [
    "4. Let’s say we have the sentence: “Deep Learning is awesome”. In a transformer encoder that uses self-attention, is the attention that the word “Learning” pays to the word ”awesome” the same as the attention the word “awesome” pays to the word “Learning”? Why or why not? (3-5 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3NMmawPcoIN"
   },
   "source": [
    "No they are different. For each word embedding in the sentence “Deep Learning is awesome”, we will extract 3 vectors: queries, keys, and values. For example, the word \"Learning\" will have three vectors: $q_{1}, \\: k_{1}$ and $v_{1}$ and \"awesome\" will have three vectors: $q_{2}, \\: k_{2}$ and $v_{2}$. In order to calculate attention \"Learning\" pays to \"awesome\", we first times $q_{1}$ with $k_{2}$ to get a score. Then we calculate the score where \"Learning\" pays to all the other words in the sentence such as \"Deep\" and \"is\". Then we divide each score by the square root of the key vector dimensionality and soft max all the scaled scores. Then we multiply the value vector with the respective soft max values. In this case for \"Learning\", the attention it pays to \"awesome\" is $\\:the\\:soft\\:max\\:value\\:$ * $v_{2}$. The attention \"awesome\" pays to \"Learning\" is $\\:the\\:soft\\:max\\:value\\:$ * $v_{1}$. Here, we can clearly see that these two equations are different because we cannot guarantee the soft max values are the same and $v_{1}$ and $v_{2}$ vectors are not the same as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAgC3BKBswWo"
   },
   "source": [
    "# 2. Sustainable Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qntta1IzuZZB"
   },
   "source": [
    "1. Recall some of the frameworks and concepts we’ve encountered so far in the course: deployment, agency, reforms, stakeholders, structures, and more. With these in mind, discuss one potential non-technical method for reducing the environmental impact of deep learning (or computationally-heavy) work, in research and/or industry settings. This could include policy approaches, economic incentive structures, academic initiatives, educational initiatives, community organizing efforts, or another theory of change. How does your approach take into account concepts of responsible computing we’ve explored so far this semester? As always, we encourage creativity and specificity! Please cite at least one source you used in forming your approach. (6-8 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TI1_8ywxuiF9"
   },
   "source": [
    "First of all, in academic institutions, if someone is writing a paper related to machine learning or deep learning,they should report training time, training hardware used, and hyperparameter sensitivity. So that when someone is reading this paper, they can have a sense of costs and benefits of training this network. Even if this model costs a lot to train and there are no methods to reduce the energy cost, one can be alarmed with this issue. Moreover, researchers should be rewarded when they train a more energy efficient model, especially in the big tech company. Although model accuracy is really important, one should also weigh the cost to achieve such high accuracy. For example, GPT-3, the AI-powered language model recently released by OpenAI, was trained on 45 terabytes of text data (the entirety of the English Wikipedia, spanning some 6 million articles, makes up only 0.6 percent of its training data). The accuracy for GPT-3 is phenomenal, but researcher should also consider the environmental price when training it. Finding a right balance is really necessary. In summary, it is really urgent to raise awareness about how costly it is to train a machine learning or deep leanring model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpxxGzWn9lkD"
   },
   "source": [
    "Citation:\n",
    "Geller, O. (2021, February 25). Is there a more environmentally friendly way to train ai? TNW | Neural. Retrieved March 26, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TbEZ4ne_ua2"
   },
   "source": [
    "# 3. CS2470-only Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttl7QNNJ_s_z"
   },
   "source": [
    "1. What requires more parameters: single or multi-headed attention? Explain. Does this mean one necessarily trains faster than another? (3-5 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7u3uTW3_0vt"
   },
   "source": [
    "Multi-headed attention requires more trainable parameters. For multi-head attention, we will train multiple attention heads and each attention head will require to train three matrices: queries, keys and values. In the end, when we concatenate all attention heads together, we will need to train another giant matrix to capture information from all the attention heads. In contrast, single head attention only need to train one attention head. This doesn't mean that multi-headed attention needs more time to train because the whole process is parallelizable. There is no sequential ordering among all the attention heads so that they can all be trained together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bYV0nq7Hkjq"
   },
   "source": [
    "2. For the last homework, we asked you to consider convolutional architectures for language modeling, and to weigh their trade-offs against RNN-based architectures. Transformers can also be used for language modeling. In fact, they are current state of the art. (see https://openai.com/blog/better-language-models/). How are transformer-based language models similar to convolution? What makes them more suited for language modeling? (5-10 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6t5pLC7bHocP"
   },
   "source": [
    "Using convolution solves the problem that training isn't parallelizable in RNN, but it still are not effective capturing long-term dependencies. Using CNN to capture long-term dependencies require many convolution layers, which will eventually make the model too large. Transformer solves the issues faced by RNN and CNN simultaneously. It completely eliminates recurrence and convolution and replace them with self attention to establish the dependencies between inputs and outputs. Self attention is a really intuitive concept that allows every element of a sequence to interact with every others and find out who they should pay more attention to. The training process for transofrmer (self attention or multi-head attention) is also parallelizable, which makes it easy to train. Thus, it is currently the best model for language modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sU92UtjLfgK"
   },
   "source": [
    "3. Read about BERT, a state-of-the-art transformer-based language model here: https://arxiv.org/pdf/1810.04805.pdf, and answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzYNp2dFNwX5"
   },
   "source": [
    "a. What do the researchers claim is novel about BERT? Why is this\n",
    "better than previous forms of language modeling techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFtqSP_ENzeB"
   },
   "source": [
    "Unlike other language modeling techniques (unidirectional), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.BERT is better because it can incorporate context from both directions. Other language model such as GPT-3 issued by OpenAI uses a left-to-right architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer, which restrict the prediction power of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvQ97rWPZF7g"
   },
   "source": [
    "b. What is the masked language model objective? Describe this in 1-2\n",
    "sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H17n0aMAZKa1"
   },
   "source": [
    "The masked language model masks out some tokens of the input and the objective is to predict the original vocabulary id of the masked word based only on its context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkghRU1cZ3ZH"
   },
   "source": [
    "c. Pretraining and finetuning are both forms of training a model. What’s\n",
    "the difference between pretraining and finetuning, and how does\n",
    "BERT use both techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wLw_zPNZ7oi"
   },
   "source": [
    "When the model is trained on a large generic corpus, it is called 'pre-training'. When it is adapted to a particular task or dataset it is called 'fine-tuning'. Fine-tuning refers to applying a pre-trained language representation to downstream tasks. For BERT, during pre-training, the model is trained on unlabeled data over different pre-training tasks. During fine-tuning, the model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSCI2470HW4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
